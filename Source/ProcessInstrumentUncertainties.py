# import python packages
import os
from abc import ABC, abstractmethod
import numpy as np
import scipy as sp
import pandas as pd
import calendar
import collections
from decimal import Decimal
from inspect import currentframe, getframeinfo
import warnings
import copy

# NPL packages
import punpy
import comet_maths as cm
import matplotlib.pyplot as plt

# HCP files
from Source import PATH_TO_CONFIG
from Source.Utilities import Utilities
from Source.ConfigFile import ConfigFile
from Source.HDFRoot import HDFRoot  # for typing
from Source.HDFGroup import HDFGroup  # for typing
from Source.ProcessL1b_FRMCal import ProcessL1b_FRMCal
from Source.Uncertainty_Analysis import Propagate
from Source.Weight_RSR import Weight_RSR
from Source.CalibrationFileReader import CalibrationFileReader
from Source.ProcessL1b_FactoryCal import ProcessL1b_FactoryCal

from Source.Uncertainty_Visualiser import UncertaintyEngine, UncertaintyGUI # class for uncertainty visualisation plots


class Instrument(ABC):
    """Base class for instrument uncertainty analysis"""

    def __init__(self):
        self.sl_method: str = 'ZONG'

    @abstractmethod
    def lightDarkStats(self, grp: HDFGroup, slice: list, sensortype: str) -> dict[np.array]:
        """
        :param grp: HDFGroup representing the sensor specific data
        :param slice: Sliced sensor data
        :param sensortype: sensor name

        :return:
        """

        pass

    def generateSensorStats(self, InstrumentType: str, rawData: dict, rawSlice: dict, newWaveBands: np.array)\
            -> dict[np.array]:
        """
        :return: dictionary of statistics used later in the processing pipeline. Keys are:
        [ave_Light, ave_Dark, std_Light, std_Dark, std_Signal]
        """
        output = {}  # used tp store standard deviations and averages as a function return for generateSensorStats
        types = ['ES', 'LI', 'LT']
        for sensortype in types:
            if InstrumentType.lower() == "trios":
                # rawData is the group and used to access _CAL, _BACK, and other information about the
                # DarkPixels... not entirely clear.
                output[sensortype] = self.lightDarkStats(
                    copy.deepcopy(rawData[sensortype]), copy.deepcopy(rawSlice[sensortype]), sensortype
                )  # copy.deepcopy ensures RAW data is unchanged for FRM uncertainty generation
            elif InstrumentType.lower() == "seabird":
                # rawData here is the group, passed along only for the purpose of
                # confirming "FrameTypes", i.e., ShutterLight or ShutterDark. Calculations
                # are performed on the Slice.
                # output contains:
                # ave_Light: (array 1 x number of wavebands)
                # ave_Dark: (array 1 x number of wavebands)
                # std_Light: (array 1 x number of wavebands)
                # std_Dark: (array 1 x number of wavebands)
                # std_Signal: OrdDict by wavebands: sqrt( (std(Light)^2 + std(Dark)^2)/ave(Light)^2 )
                output[sensortype] = self.lightDarkStats([rawData[sensortype]['LIGHT'],
                                                          rawData[sensortype]['DARK']],
                                                         [rawSlice[sensortype]['LIGHT'],
                                                          rawSlice[sensortype]['DARK']],
                                                         sensortype)
        if not output[sensortype]:
            msg = "Error in generating standard deviation and average of light and dark"
            print(msg)
            return False

        # interpolate std Signal to common wavebands for use in band convolution
        for stype in types:
            _, output[stype]['std_Signal_Interpolated'] = self.interp_common_wvls(
                output[stype]['std_Signal'],
                np.asarray(list(output[stype]['std_Signal'].keys()), dtype=float),
                newWaveBands)

        return output

    def Factory(self, node: HDFRoot, uncGrp: HDFGroup, stats: dict) -> dict[str: dict]:
        """

        :param node: HDFRoot which stores the L1BQC data for L2Processing
        :param uncGrp: HDFGroup which contains the uncertainty budget, all input uncertainties
        :param stats: dictionary generated by Source.ProcessInstrumentUncertainties.generateSensorStats() contains
        sensor specific standard deviations and averages for the light, dark and light-dark signals.

        :return: dictionary of output instrument uncertainties [ES, LI, LT]
        """
        # read in uncertainties from HDFRoot and define propagate object
        PropagateL1B = Propagate(M=100, cores=0)

        # define dictionaries for uncertainty components
        Cal = {}
        Coeff = {}
        cPol = {}
        cStray = {}
        Ct = {}
        cLin = {}
        cStab = {}

        # loop through instruments
        for sensor in ["ES", "LI", "LT"]:
            radcal = uncGrp.getDataset(f"{sensor}_RADCAL_UNC") # SIRREX data
            radcal.datasetToColumns()

            # added attribute in node for Factory branch which accounts for missing calibration data.
            cal_start = int(node.attributes['CAL_START'])
            cal_stop = int(node.attributes['CAL_STOP'])
            straylight = uncGrp.getDataset(f"{sensor}_STRAYDATA_CAL")
            straylight.datasetToColumns()
            cStray[sensor] = np.asarray(list(straylight.columns['1']))[cal_start:cal_stop + 1]
            # +1 here fixed a bug. Slicing arrays gives the first stop-start elements, not elements up to stop index.
            # Therefore for 255 pixels we need 0:255 not 0:254 to capture all pixels in the sl values.

            linear = uncGrp.getDataset(sensor + "_NLDATA_CAL")
            linear.datasetToColumns()
            cLin[sensor] = np.asarray(list(linear.columns['1']))

            stab = uncGrp.getDataset(sensor + "_STABDATA_CAL")
            stab.datasetToColumns()
            cStab[sensor] = np.asarray(list(stab.columns['1']))

            ### ADERU : Read radiometric coeff value from configuration files
            Cal[sensor] = np.asarray(list(radcal.columns['unc']))
            calFolder = os.path.splitext(ConfigFile.filename)[0] + "_Calibration"
            calPath = os.path.join(PATH_TO_CONFIG, calFolder)
            calibrationMap = CalibrationFileReader.read(calPath)
            waves, Coeff[sensor] = ProcessL1b_FactoryCal.extract_calibration_coeff(node, calibrationMap, sensor)

            if waves is None and Coeff is None:
                msg = 'ProcessInstrumentUncertainties Fail'
                print(msg)
                Utilities.writeLogFile(msg)
                return None

            # temporary fix angular for ES is written as ES_POL
            pol = uncGrp.getDataset(sensor + "_POLDATA_CAL")
            pol.datasetToColumns()
            cPol[sensor] = np.asarray(list(pol.columns['1']))

            # temp uncertainties calculated at L1AQC
            Temp = uncGrp.getDataset(sensor + "_TEMPDATA_CAL")
            Temp.datasetToColumns()
            Ct[sensor] = np.array(Temp.columns[f'{sensor}_TEMPERATURE_UNCERTAINTIES'])

        ones = np.ones(len(Cal['ES']))  # to provide array of 1s with the correct shape

        # sensor specific behaviour handled in ProcessInstrumentUncertainties.LightDarkStats()

        # create lists containing mean values and their associated uncertainties (list order matters)
        mean_values = [stats['ES']['ave_Light'], stats['ES']['ave_Dark'],
                       stats['LI']['ave_Light'], stats['LI']['ave_Dark'],
                       stats['LT']['ave_Light'], stats['LT']['ave_Dark'],
                       Coeff['ES'], Coeff['LI'], Coeff['LT'],
                       ones, ones, ones,
                       ones, ones, ones,
                       ones, ones, ones,
                       ones, ones, ones,
                       ones, ones, ones]

        uncertainty = [stats['ES']['std_Light'], stats['ES']['std_Dark'],
                       stats['LI']['std_Light'], stats['LI']['std_Dark'],
                       stats['LT']['std_Light'], stats['LT']['std_Dark'],
                       Cal['ES']*Coeff['ES']/200, Cal['LI']*Coeff['LI']/200, Cal['LT']*Coeff['LT']/200,
                       cStab['ES'], cStab['LI'], cStab['LT'],
                       cLin['ES'], cLin['LI'], cLin['LT'],
                       np.array(cStray['ES'])/100, np.array(cStray['LI'])/100, np.array(cStray['LT'])/100,
                       np.array(Ct['ES']), np.array(Ct['LI']), np.array(Ct['LT']),
                       np.array(cPol['LI']), np.array(cPol['LT']), np.array(cPol['ES'])]

        # generate uncertainties using Monte Carlo Propagation (M=100, def line 27)
        es_unc, li_unc, lt_unc = PropagateL1B.propagate_Instrument_Uncertainty(mean_values, uncertainty)
        es, li, lt = PropagateL1B.instruments(*mean_values)  # signal generated from measurement function applied
        # in punpy call, so uncertainties are now relative to what means are provided in mean_values
        # convert to relative uncertainty
        # For calibrations with zeroes for Coeff (i.e., lamp constraints), this will yield NaN uncertainties
        # Temporary workaround (results in zeroes instead)
        # es[es==0] = 1
        # li[li==0] = 1
        # lt[lt==0] = 1
        # Restored to yield NaN uncertainties in uncharacterized bands
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", message="invalid value encountered in divide")
            ES_unc = es_unc / es
            LI_unc = li_unc / li
            LT_unc = lt_unc / lt  # when converted back to absolute in ProcessL2, they will be converted to the same units
                                # as ES, lI, & LT respectively.

        # return uncertainties as dictionary to be appended to xSlice
        data_wvl = np.asarray(list(stats['ES']['std_Signal_Interpolated'].keys()), dtype=float)  # std_Signal_Interpolated has keys which represent common wavebands for ES, LI, & LT.
        _, es_Unc = self.interp_common_wvls(ES_unc,
                                            np.array(uncGrp.getDataset("ES_RADCAL_UNC").columns['wvl'], dtype=float),
                                            data_wvl)
        _, li_Unc = self.interp_common_wvls(LI_unc,
                                            np.array(uncGrp.getDataset("LI_RADCAL_UNC").columns['wvl'], dtype=float),
                                            data_wvl)
        _, lt_Unc = self.interp_common_wvls(LT_unc,
                                            np.array(uncGrp.getDataset("LT_RADCAL_UNC").columns['wvl'], dtype=float),
                                            data_wvl)

        # plot class based L1B uncertainties
        if ConfigFile.settings['bL2UncertaintyBreakdownPlot']:
            # if I understand how the stations group in the L1BQC hdf works then this should always be a list with one
            # value
            stations = np.array(node.getGroup("ANCILLARY").getDataset("STATION").columns["STATION"])
            if stations is not None:
                cast = (f"{type(self).__name__}_{node.attributes['CAST']}_"
                        f"{np.unique(stations[~np.isnan(stations)]).tolist()[0]}")
            else:
                cast = f"{type(self).__name__}_{node.attributes['CAST']}"

            p_unc = UncertaintyGUI(PropagateL1B)
            p_unc.pie_plot_class(
                mean_values,
                uncertainty,
                dict(
                    ES=waves,
                    LI=waves,
                    LT=waves
                ),
                cast,
                node.getGroup("ANCILLARY")
            )
            p_unc.plot_class(
                mean_values,
                uncertainty,
                dict(
                    ES=waves,
                    LI=waves,
                    LT=waves
                    ),
                cast
            )

        return dict(
            esUnc=es_Unc,
            liUnc=li_Unc,
            ltUnc=lt_Unc,
        )

    def Default(self, uncGrp: HDFGroup, stats: dict, node: HDFRoot) -> dict[str, dict]:
        """

        :param uncGrp: HDFGroup which contains the uncertainty budget, all imput uncertainties
        :param stats: dictionary generated by Source.ProcessInstrumentUncertainties.generateSensorStats() contains
        sensor specific standard deviations and averages for the light, dark and light-dark signals.

        :return: dictionary of output instrument uncertainties [Es_unc, Li_unc, Lt_unc]
        """
        # read in uncertainties from HDFRoot and define propagate object
        PropagateL1B = Propagate(M=100, cores=0)

        # define dictionaries for uncertainty components
        Cal = {}
        Coeff = {}
        cPol = {}
        cStray = {}
        Ct = {}
        cLin = {}
        cStab = {}

        # loop through instruments
        for sensor in ["ES", "LI", "LT"]:
            radcal = uncGrp.getDataset(f"{sensor}_RADCAL_CAL")
            radcal.datasetToColumns()
            ind_rad_wvl = (np.array(radcal.columns['1']) > 0)

            straylight = uncGrp.getDataset(f"{sensor}_STRAYDATA_CAL")
            straylight.datasetToColumns()
            cStray[sensor] = np.asarray(list(straylight.columns['1']))[ind_rad_wvl]

            linear = uncGrp.getDataset(sensor + "_NLDATA_CAL")
            linear.datasetToColumns()
            cLin[sensor] = np.asarray(list(linear.columns['1']))

            stab = uncGrp.getDataset(sensor + "_STABDATA_CAL")
            stab.datasetToColumns()
            cStab[sensor] = np.asarray(list(stab.columns['1']))

            if ConfigFile.settings['SensorType'].lower() == "trios":
                # Convert TriOS mW/m2/nm to uW/cm^2/nm
                Coeff[sensor] = np.asarray(list(radcal.columns['2']))[ind_rad_wvl]/10
            elif ConfigFile.settings['SensorType'].lower() == "seabird":
                Coeff[sensor] = np.asarray(list(radcal.columns['2']))[ind_rad_wvl]
            Cal[sensor] = np.asarray(list(radcal.columns['3']))[ind_rad_wvl]

            # temporary fix angular for ES is written as ES_POL
            pol = uncGrp.getDataset(sensor + "_POLDATA_CAL")
            pol.datasetToColumns()
            cPol[sensor] = np.asarray(list(pol.columns['1']))

            # temp uncertainties calculated at L1AQC
            Temp = uncGrp.getDataset(sensor + "_TEMPDATA_CAL")
            Temp.datasetToColumns()
            Ct[sensor] = np.array(Temp.columns[f'{sensor}_TEMPERATURE_UNCERTAINTIES'])

        ones = np.ones(len(Cal['ES']))  # to provide array of 1s with the correct shape

        # sensor specific behaviour handled in ProcessInstrumentUncertainties.LightDarkStats()

        # create lists containing mean values and their associated uncertainties (list order matters)
        mean_values = [stats['ES']['ave_Light'], stats['ES']['ave_Dark'],
                       stats['LI']['ave_Light'], stats['LI']['ave_Dark'],
                       stats['LT']['ave_Light'], stats['LT']['ave_Dark'],
                       Coeff['ES'], Coeff['LI'], Coeff['LT'],
                       ones, ones, ones,
                       ones, ones, ones,
                       ones, ones, ones,
                       ones, ones, ones,
                       ones, ones, ones
                       ]

        uncertainty = [stats['ES']['std_Light'], stats['ES']['std_Dark'],
                       stats['LI']['std_Light'], stats['LI']['std_Dark'],
                       stats['LT']['std_Light'], stats['LT']['std_Dark'],
                       Cal['ES']*Coeff['ES']/200,
                       Cal['LI']*Coeff['LI']/200,
                       Cal['LT']*Coeff['LT']/200,
                       cStab['ES'], cStab['LI'], cStab['LT'],
                       cLin['ES'], cLin['LI'], cLin['LT'],
                       np.array(cStray['ES'])/100,
                       np.array(cStray['LI'])/100,
                       np.array(cStray['LT'])/100,
                       np.array(Ct['ES']), np.array(Ct['LI']), np.array(Ct['LT']),
                       np.array(cPol['LI']), np.array(cPol['LT']), np.array(cPol['ES'])
                       ]

        # generate uncertainties using Monte Carlo Propagation (M=100, def line 27)
        es_unc, li_unc, lt_unc = PropagateL1B.propagate_Instrument_Uncertainty(mean_values, uncertainty)
        es, li, lt = PropagateL1B.instruments(*mean_values)  # signal generated from measurement function applied
        # in punpy call, so uncertainties are now relative to what means are provided in mean_values
        # convert to relative uncertainty

        # plot class based L1B uncertainties
        if ConfigFile.settings['bL2UncertaintyBreakdownPlot']:
            # if I understand how the stations group in the L1BQC hdf works then this should always be a list with one
            # value
            stations = np.array(node.getGroup("ANCILLARY").getDataset("STATION").columns["STATION"])
            if stations is not None:
                cast = (f"{type(self).__name__}_{node.attributes['CAST']}_"
                        f"{np.unique(stations[~np.isnan(stations)]).tolist()[0]}")
            else:
                cast = f"{type(self).__name__}_{node.attributes['CAST']}"

            p_unc = UncertaintyGUI(PropagateL1B)
            p_unc.pie_plot_class(
                mean_values,
                uncertainty,
                dict(
                    ES=np.array(uncGrp.getDataset("ES_RADCAL_CAL").columns['1']),
                    LI=np.array(uncGrp.getDataset("LI_RADCAL_CAL").columns['1']),
                    LT=np.array(uncGrp.getDataset("LT_RADCAL_CAL").columns['1'])
                ),
                cast,
                node.getGroup("ANCILLARY")
            )
            p_unc.plot_class(
                mean_values,
                uncertainty,
                dict(
                    ES=np.array(uncGrp.getDataset("ES_RADCAL_CAL").columns['1']),
                    LI=np.array(uncGrp.getDataset("LI_RADCAL_CAL").columns['1']),
                    LT=np.array(uncGrp.getDataset("LT_RADCAL_CAL").columns['1'])
                    ),
                cast
            )

        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", message="invalid value encountered in divide")
            ES_unc = es_unc / es
            LI_unc = li_unc / li
            LT_unc = lt_unc / lt  # when converted back to absolute in ProcessL2, they will be converted to the same units
            # as ES, lI, & LT respectively.

        # return uncertainties as dictionary to be appended to xSlice
        data_wvl = np.asarray(list(stats['ES']['std_Signal_Interpolated'].keys()), dtype=float)  # std_Signal_Interpolated has keys which represent common wavebands for ES, LI, & LT.
        _, es_Unc = self.interp_common_wvls(ES_unc,
                                            np.array(uncGrp.getDataset("ES_RADCAL_CAL").columns['1'], dtype=float)[ind_rad_wvl],
                                            data_wvl)
        _, li_Unc = self.interp_common_wvls(LI_unc,
                                            np.array(uncGrp.getDataset("LI_RADCAL_CAL").columns['1'], dtype=float)[ind_rad_wvl],
                                            data_wvl)
        _, lt_Unc = self.interp_common_wvls(LT_unc,
                                            np.array(uncGrp.getDataset("LT_RADCAL_CAL").columns['1'], dtype=float)[ind_rad_wvl],
                                            data_wvl)

        # radcal_cal = pd.DataFrame(uncGrp.getDataset(sensor + "_RADCAL_CAL").data)['2']
        #
        # ind_zero = radcal_cal <= 0
        # ind_nan = np.isnan(radcal_cal)
        # ind_nocal = ind_nan | ind_zero
        #
        # for i, k in enumerate(es_Unc.keys()):
        #     if ind_nocal[i]:
        #         es_Unc[k] = [0.0]
        #         li_Unc[k] = [0.0]
        #         lt_Unc[k] = [0.0]

        return dict(
            esUnc=es_Unc,
            liUnc=li_Unc,
            ltUnc=lt_Unc,
        )

    @abstractmethod
    def FRM(self, node: HDFRoot, uncGrp: HDFGroup, raw_grps: dict[str, HDFGroup], raw_slices: dict[str, np.array],
            stats: dict, newWaveBands: np.array) -> dict[str, np.array]:
        """
        :param node: HDFRoot of L1BQC data for procressing
        :param uncGrp: HDFGroup of uncertainty budget
        :param raw_grps: dictionary of raw data groups
        :param raw_slices: dictionary of sliced data for specific sensors
        :param stats: standard deviation and averages for Light, Dark and Light-Dark signal
        :param newWaveBands: wavelength subset for interpolation

        :return: output FRM uncertainties
        """
        pass

    ## L2 uncertainty Processing
    @staticmethod
    def rrsHyperUNCFRM(rhoScalar: float, rhoVec: np.array, rhoDelta: np.array, waveSubset: np.array,
                       xSlice: dict[str, np.array]) -> dict[str, np.array]:
        """
        :param rhoScalar: rho input if Mobley99 or threeC rho is used
        :param rhoVec: rho input if Zhang17 rho is used
        :param rhoDelta: uncertainties associated with rho
        :param waveSubset: wavelength subset for any band convolution (and sizing rhoScalar if used)
        :param xSlice: Dictionary of input radiance, raw_counts, standard deviations etc.

        :return: dictionary of output uncertainties that are generated

        """
        # organise data
        # cut data down to wavelengths where rho values exist -- should be no change for M99
        esSampleXSlice = np.asarray([{key: sample for key, sample in
                                      xSlice['esSample'][i].items() if float(key) in waveSubset}
                                     for i in range(len(xSlice['esSample']))])
        liSampleXSlice = np.asarray([{key: sample for key, sample in
                                      xSlice['liSample'][i].items() if float(key) in waveSubset}
                                     for i in range(len(xSlice['liSample']))])
        ltSampleXSlice = np.asarray([{key: sample for key, sample in
                                      xSlice['ltSample'][i].items() if float(key) in waveSubset}
                                     for i in range(len(xSlice['ltSample']))])

        if rhoScalar is not None:  # make rho a constant array if scalar
            rho = np.ones(len(waveSubset))*rhoScalar  # convert rhoScalar to the same dims as other values/Uncertainties
        else:
            rho = np.asarray(list(rhoVec.values()), dtype=float)

        # initialise punpy propagation object
        mdraws = esSampleXSlice.shape[0]  # keep no. of monte carlo draws consistent
        Propagate_L2_FRM = punpy.MCPropagation(mdraws, parallel_cores=1)

        # get sample for rho
        rhoSample = cm.generate_sample(mdraws, rho, rhoDelta, "syst")

        # initialise lists to store uncertainties per replicate

        esSample = np.asarray([[i[0] for i in k.values()] for k in esSampleXSlice])  # recover original shape of samples
        liSample = np.asarray([[i[0] for i in k.values()] for k in liSampleXSlice])
        ltSample = np.asarray([[i[0] for i in k.values()] for k in ltSampleXSlice])

        # no uncertainty in wvls
        sample_wavelengths = cm.generate_sample(mdraws, np.array(waveSubset), None, None)
        sample_Lw = Propagate_L2_FRM.run_samples(Propagate.Lw_FRM, [ltSample, rhoSample, liSample])
        sample_Rrs = Propagate_L2_FRM.run_samples(Propagate.Rrs_FRM, [ltSample, rhoSample, liSample, esSample])

        output = {}

        if ConfigFile.settings["bL2WeightSentinel3A"]:
            # changes made here should not affect output uncertainties, but will give us more control over how
            # uncertainty components such as esUncSlice are convolved. (We were missing a small amount of convolution
            # uncertainty before!
            sample_es_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_S3A, [esSample, sample_wavelengths])
            sample_li_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_S3A, [liSample, sample_wavelengths])
            sample_lt_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_S3A, [ltSample, sample_wavelengths])

            sample_rho_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_S3A, [rhoSample, sample_wavelengths])

            esDeltaBand = Propagate_L2_FRM.process_samples(None, sample_es_S3A)
            liDeltaBand = Propagate_L2_FRM.process_samples(None, sample_li_S3A)
            ltDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lt_S3A)

            rhoDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rho_S3A)

            # sample_lw_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_S3A, [sample_Lw, sample_wavelengths])
            # sample_rrs_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_S3A, [sample_Rrs, sample_wavelengths])

            # rrs and lw samples now derrived from running convolved instrument data through LW and Rrs measurement funcs
            # should be a time save vs running the band convolution code again!
            sample_lw_S3A = Propagate_L2_FRM.run_samples(Propagate.Lw_FRM, [sample_lt_S3A,
                                                                            sample_rho_S3A,
                                                                            sample_li_S3A
                                                                            ])

            sample_rrs_S3A = Propagate_L2_FRM.run_samples(Propagate.Rrs_FRM, [sample_lt_S3A,
                                                                              sample_rho_S3A,
                                                                              sample_li_S3A,
                                                                              sample_es_S3A
                                                                              ])

            lwDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lw_S3A)
            rrsDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rrs_S3A)

            # put in expected format (converted from punpy conpatible outputs) and put in output dictionary which will
            # be returned to ProcessingL2 and used to update xSlice/xUNC
            output["esUNC_Sentinel3A"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), esDeltaBand)}
            output["liUNC_Sentinel3A"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), liDeltaBand)}
            output["ltUNC_Sentinel3A"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), ltDeltaBand)}
            output["rhoUNC_Sentinel3A"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), rhoDeltaBand)}
            output["lwUNC_Sentinel3A"] = lwDeltaBand
            output["rrsUNC_Sentinel3A"] = rrsDeltaBand  # L2 uncertainty products can be reported as np arrays

        if ConfigFile.settings["bL2WeightSentinel3B"]:
            sample_es_S3B = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_S3B, [esSample, sample_wavelengths])
            sample_li_S3B = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_S3B, [liSample, sample_wavelengths])
            sample_lt_S3B = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_S3B, [ltSample, sample_wavelengths])

            sample_rho_S3B = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_S3B,
                                                          [rhoSample, sample_wavelengths])

            esDeltaBand = Propagate_L2_FRM.process_samples(None, sample_es_S3B)
            liDeltaBand = Propagate_L2_FRM.process_samples(None, sample_li_S3B)
            ltDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lt_S3B)

            rhoDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rho_S3B)

            # sample_lw_S3B = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_S3B, [sample_Lw, sample_wavelengths])
            # sample_rrs_S3B = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_S3B, [sample_Rrs, sample_wavelengths])

            sample_lw_S3B = Propagate_L2_FRM.run_samples(Propagate.Lw_FRM, [sample_lt_S3B,
                                                                                  sample_rho_S3B,
                                                                                  sample_li_S3B
                                                                                 ])

            sample_rrs_S3B = Propagate_L2_FRM.run_samples(Propagate.Rrs_FRM, [sample_lt_S3B,
                                                                                    sample_rho_S3B,
                                                                                    sample_li_S3B,
                                                                                    sample_es_S3B
                                                                                   ])

            lwDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lw_S3B)
            rrsDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rrs_S3B)

            output["esUNC_Sentinel3B"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), esDeltaBand)}
            output["liUNC_Sentinel3B"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), liDeltaBand)}
            output["ltUNC_Sentinel3B"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), ltDeltaBand)}
            output["rhoUNC_Sentinel3B"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), rhoDeltaBand)}
            output["lwUNC_Sentinel3B"] = lwDeltaBand
            output["rrsUNC_Sentinel3B"] = rrsDeltaBand

        if ConfigFile.settings['bL2WeightMODISA']:

            sample_es_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_AQUA, [esSample, sample_wavelengths])
            sample_li_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_AQUA, [liSample, sample_wavelengths])
            sample_lt_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_AQUA, [ltSample, sample_wavelengths])

            sample_rho_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_AQUA,
                                                          [rhoSample, sample_wavelengths])

            esDeltaBand = Propagate_L2_FRM.process_samples(None, sample_es_S3A)
            liDeltaBand = Propagate_L2_FRM.process_samples(None, sample_li_S3A)
            ltDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lt_S3A)

            rhoDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rho_S3A)

            sample_lw_AQUA = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_AQUA, [sample_Lw, sample_wavelengths])
            sample_rrs_AQUA = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_AQUA, [sample_Rrs, sample_wavelengths])

            lwDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lw_AQUA)
            rrsDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rrs_AQUA)

            output["esUNC_MODISA"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), esDeltaBand)}
            output["liUNC_MODISA"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), liDeltaBand)}
            output["ltUNC_MODISA"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), ltDeltaBand)}
            output["rhoUNC_MODISA"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), rhoDeltaBand)}
            output["lwUNC_MODISA"] = lwDeltaBand
            output["rrsUNC_MODISA"] = rrsDeltaBand

        if ConfigFile.settings['bL2WeightMODIST']:

            sample_es_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_TERRA,
                                                         [esSample, sample_wavelengths])
            sample_li_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_TERRA,
                                                         [liSample, sample_wavelengths])
            sample_lt_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_TERRA,
                                                         [ltSample, sample_wavelengths])

            sample_rho_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_TERRA,
                                                          [rhoSample, sample_wavelengths])

            esDeltaBand = Propagate_L2_FRM.process_samples(None, sample_es_S3A)
            liDeltaBand = Propagate_L2_FRM.process_samples(None, sample_li_S3A)
            ltDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lt_S3A)

            rhoDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rho_S3A)

            sample_lw_TERRA = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_TERRA, [sample_Lw, sample_wavelengths])
            sample_rrs_TERRA = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_TERRA, [sample_Rrs, sample_wavelengths])

            lwDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lw_TERRA)
            rrsDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rrs_TERRA)

            output["esUNC_MODIST"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), esDeltaBand)}
            output["liUNC_MODIST"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), liDeltaBand)}
            output["ltUNC_MODIST"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), ltDeltaBand)}
            output["rhoUNC_MODIST"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), rhoDeltaBand)}
            output["lwUNC_MODIST"] = lwDeltaBand
            output["rrsUNC_MODIST"] = rrsDeltaBand

        if ConfigFile.settings['bL2WeightVIIRSN']:

            sample_es_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_NOAA_N,
                                                         [esSample, sample_wavelengths])
            sample_li_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_NOAA_N,
                                                         [liSample, sample_wavelengths])
            sample_lt_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_NOAA_N,
                                                         [ltSample, sample_wavelengths])

            sample_rho_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_NOAA_N,
                                                          [rhoSample, sample_wavelengths])

            esDeltaBand = Propagate_L2_FRM.process_samples(None, sample_es_S3A)
            liDeltaBand = Propagate_L2_FRM.process_samples(None, sample_li_S3A)
            ltDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lt_S3A)

            rhoDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rho_S3A)

            sample_lw_NOAA = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_NOAA_N, [sample_Lw, sample_wavelengths])
            sample_rrs_NOAA = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_NOAA_N, [sample_Rrs, sample_wavelengths])

            lwDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lw_NOAA)
            rrsDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rrs_NOAA)

            output["esUNC_VIIRSN"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), esDeltaBand)}
            output["liUNC_VIIRSN"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), liDeltaBand)}
            output["ltUNC_VIIRSN"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), ltDeltaBand)}
            output["rhoUNC_VIIRSN"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), rhoDeltaBand)}
            output["lwUNC_VIIRSN"] = lwDeltaBand
            output["rrsUNC_VIIRSN"] = rrsDeltaBand

        if ConfigFile.settings['bL2WeightVIIRSJ']:

            sample_es_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_NOAA_J,
                                                         [esSample, sample_wavelengths])
            sample_li_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_NOAA_J,
                                                         [liSample, sample_wavelengths])
            sample_lt_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_NOAA_J,
                                                         [ltSample, sample_wavelengths])

            sample_rho_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_NOAA_J,
                                                          [rhoSample, sample_wavelengths])

            esDeltaBand = Propagate_L2_FRM.process_samples(None, sample_es_S3A)
            liDeltaBand = Propagate_L2_FRM.process_samples(None, sample_li_S3A)
            ltDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lt_S3A)

            rhoDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rho_S3A)

            sample_lw_NOAAJ = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_NOAA_J, [sample_Lw, sample_wavelengths])
            sample_rrs_NOAAJ = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_NOAA_J, [sample_Rrs, sample_wavelengths])

            lwDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lw_NOAAJ)
            rrsDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rrs_NOAAJ)

            output["esUNC_VIIRSJ"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), esDeltaBand)}
            output["liUNC_VIIRSJ"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), liDeltaBand)}
            output["ltUNC_VIIRSJ"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), ltDeltaBand)}
            output["rhoUNC_VIIRSJ"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), rhoDeltaBand)}
            output["lwUNC_VIIRSJ"] = lwDeltaBand
            output["rrsUNC_VIIRSJ"] = rrsDeltaBand

        lwDelta = Propagate_L2_FRM.process_samples(None, sample_Lw)
        rrsDelta = Propagate_L2_FRM.process_samples(None, sample_Rrs)

        output["rhoUNC_HYPER"] = {str(wvl): val for wvl, val in zip(waveSubset, rhoDelta)}
        output["lwUNC"] = lwDelta  # Multiply by large number to reduce round off error
        output["rrsUNC"] = rrsDelta

        return output

    def rrsHyperUNC(self, node, uncGrp: HDFGroup, rhoScalar: float, rhoVec: np.array, rhoDelta: np.array,
                    waveSubset: np.array, xSlice: dict[str, np.array]) -> dict[str, np.array]:
        """
        :param node: HDFRoot
        :param uncGrp: HDFGroup storing the uncertainty budget
        :param rhoScalar: rho input if Mobley99 or threeC rho is used
        :param rhoVec: rho input if Zhang17 rho is used
        :param rhoDelta: uncertainties associated with rho
        :param waveSubset: wavelength subset for any band convolution (and sizing rhoScalar if used)
        :param xSlice: Dictionary of input radiance, raw_counts, standard deviations etc.

        :return: dictionary of output uncertainties that are generated
        """

        waveSubset = np.array(waveSubset, dtype=float)  # convert waveSubset to numpy array
        esXstd = xSlice['esSTD_RAW']
        liXstd = xSlice['liSTD_RAW']
        ltXstd = xSlice['ltSTD_RAW']

        if rhoScalar is not None:  # make rho a constant array if scalar
            rho = np.ones(len(list(esXstd.keys())))*rhoScalar
            rhoUNC, _ = self.interp_common_wvls(np.array(rhoDelta, dtype=float),
                                                waveSubset,
                                                np.asarray(list(esXstd.keys()), dtype=float))
        else:
            rho, _ = self.interp_common_wvls(np.array(list(rhoVec.values()), dtype=float),
                                             waveSubset,
                                             np.asarray(list(esXstd.keys()), dtype=float))
            rhoUNC, _ = self.interp_common_wvls(rhoDelta,
                                                waveSubset,
                                                np.asarray(list(esXstd.keys()), dtype=float))

        # define dictionaries for uncertainty components
        Cal = {}
        Coeff = {}
        cPol = {}
        cStray = {}
        Ct = {}
        cLin = {}
        cStab = {}

        for sensor in ["ES", "LI", "LT"]:
            radcal = uncGrp.getDataset(f"{sensor}_RADCAL_CAL")
            radcal.datasetToColumns()
            ind_rad_wvl = (np.array(radcal.columns['1']) > 0)

            straylight = uncGrp.getDataset(f"{sensor}_STRAYDATA_CAL")
            straylight.datasetToColumns()
            cStray[sensor] = np.asarray(list(straylight.columns['1']))[ind_rad_wvl]

            linear = uncGrp.getDataset(sensor + "_NLDATA_CAL")
            linear.datasetToColumns()
            cLin[sensor] = np.asarray(list(linear.columns['1']))

            stab = uncGrp.getDataset(sensor + "_STABDATA_CAL")
            stab.datasetToColumns()
            cStab[sensor] = np.asarray(list(stab.columns['1']))

            if ConfigFile.settings['SensorType'].lower() == "trios":
                # Convert TriOS mW/m2/nm to uW/cm^2/nm
                Coeff[sensor] = np.asarray(list(radcal.columns['2']))[ind_rad_wvl]/10
            elif ConfigFile.settings['SensorType'].lower() == "seabird":
                Coeff[sensor] = np.asarray(list(radcal.columns['2']))[ind_rad_wvl]
            Cal[sensor] = np.asarray(list(radcal.columns['3']))[ind_rad_wvl]

            pol = uncGrp.getDataset(sensor + "_POLDATA_CAL")
            pol.datasetToColumns()
            cPol[sensor] = np.asarray(list(pol.columns['1']))  # es cos stored in poldata file as temporary fix

            # temp uncertainties calculated at L1AQC
            Temp = uncGrp.getDataset(sensor + "_TEMPDATA_CAL")
            Temp.datasetToColumns()
            Ct[sensor] = np.array(Temp.columns[f'{sensor}_TEMPERATURE_UNCERTAINTIES'])

        # moved here so ind_rad_wvl exists for masking
        es, _ = self.interp_common_wvls(np.asarray(list(xSlice['es'].values()), dtype=float).flatten(),
                                        np.asarray(list(xSlice['es'].keys()), dtype=float).flatten(),
                                        np.array(uncGrp.getDataset("ES_RADCAL_CAL").columns['1'], dtype=float)[ind_rad_wvl])
        li, _ = self.interp_common_wvls(np.asarray(list(xSlice['li'].values()), dtype=float).flatten(),
                                        np.asarray(list(xSlice['li'].keys()), dtype=float).flatten(),
                                        np.array(uncGrp.getDataset("LI_RADCAL_CAL").columns['1'], dtype=float)[ind_rad_wvl])
        lt, _ = self.interp_common_wvls(np.asarray(list(xSlice['lt'].values()), dtype=float).flatten(),
                                        np.asarray(list(xSlice['lt'].keys()), dtype=float).flatten(),
                                        np.array(uncGrp.getDataset("LT_RADCAL_CAL").columns['1'], dtype=float)[ind_rad_wvl])

        Propagate_L2 = Propagate(M=100, cores=0)
        slice_size = len(es)
        ones = np.ones(slice_size)
        # zeros = np.zeros(slice_size)

        lw_means = [lt, rho, li,
                    ones, ones,
                    ones, ones,
                    ones, ones,
                    ones, ones,
                    ones, ones,
                    ones, ones]

        lw_uncertainties = [np.abs(np.array(list(ltXstd.values())).flatten() * lt),
                            rhoUNC,
                            np.abs(np.array(list(liXstd.values())).flatten() * li),
                            Cal['LI']/200, Cal['LT']/200,
                            cStab['LI'], cStab['LT'],
                            cLin['LI'], cLin['LT'],
                            cStray['LI']/100, cStray['LI']/100,
                            Ct['LI'], Ct['LI'],
                            cPol['LI'], cPol['LI']]

        lwAbsUnc = Propagate_L2.Propagate_Lw_HYPER(lw_means, lw_uncertainties)
        lw_vals = Propagate_L2.Lw(*lw_means)

        rrs_means = [lt, rho, li, es,
                     ones, ones, ones,
                     ones, ones, ones,
                     ones, ones, ones,
                     ones, ones, ones,
                     ones, ones, ones,
                     ones, ones, ones
                     ]

        rrs_uncertainties = [np.abs(np.array(list(ltXstd.values())).flatten() * lt),
                             rhoUNC,
                             np.abs(np.array(list(liXstd.values())).flatten() * li),
                             np.abs(np.array(list(esXstd.values())).flatten() * es),
                             Cal['ES']/200, Cal['LI']/200, Cal['LT']/200,
                             cStab['ES'], cStab['LI'], cStab['LT'],
                             cLin['ES'], cLin['LI'], cLin['LT'],
                             cStray['ES']/100, cStray['LI']/100, cStray['LT']/100,
                             Ct['ES'], Ct['LI'], Ct['LT'],
                             cPol['LI'], cPol['LT'], cPol['ES']
                             ]

        rrsAbsUnc = Propagate_L2.Propagate_RRS_HYPER(rrs_means, rrs_uncertainties)
        rrs_vals = Propagate_L2.RRS(*rrs_means)

        # Plot Class based L2 uncertainties
        if ConfigFile.settings['bL2UncertaintyBreakdownPlot']:
            # if I understand how the stations group in the L1BQC hdf works then this should always be a list with one
            # value
            stations = np.array(node.getGroup("ANCILLARY").getDataset("STATION").columns["STATION"])
            if stations is not None:
                cast = (f"{type(self).__name__}_{node.attributes['CAST']}_"
                        f"{np.unique(stations[~np.isnan(stations)]).tolist()[0]}")
            else:
                cast = f"{type(self).__name__}_{node.attributes['CAST']}"

            p_unc = UncertaintyGUI()
            p_unc.pie_plot_class_l2(
                rrs_means,
                lw_means,
                rrs_uncertainties,
                lw_uncertainties,
                np.array(uncGrp.getDataset("ES_RADCAL_CAL").columns['1'], dtype=float),  # pass radcal wavelengths
                cast,
                node.getGroup("ANCILLARY")
            )

            p_unc.plot_class_L2(
                rrs_means,
                lw_means,
                rrs_uncertainties,
                lw_uncertainties,
                np.array(uncGrp.getDataset("ES_RADCAL_CAL").columns['1'], dtype=float),
                cast
            )

        ## BAND CONVOLUTION
        # band convolution of uncertainties is done here to include uncertainty contribution of band convolution process
        Convolve = Propagate(M=100, cores=1)
        # these are absolute values! Dont get confused
        output = {}

        # interpolate output uncertainties to the waveSubset (common wavebands of interpolated es, li, & lt)
        # wvls = np.asarray(list(xSlice['es'].keys()), dtype=float)

        lwAbsUnc, _ = self.interp_common_wvls(lwAbsUnc,
                                              np.array(uncGrp.getDataset("ES_RADCAL_CAL").columns['1'], dtype=float)[ind_rad_wvl],
                                              waveSubset)
        lw_vals, _ = self.interp_common_wvls(lw_vals,
                                             np.array(uncGrp.getDataset("ES_RADCAL_CAL").columns['1'], dtype=float)[ind_rad_wvl],
                                             waveSubset)
        rrsAbsUnc, _ = self.interp_common_wvls(rrsAbsUnc,
                                               np.array(uncGrp.getDataset("ES_RADCAL_CAL").columns['1'], dtype=float)[ind_rad_wvl],
                                               waveSubset)
        rrs_vals, _ = self.interp_common_wvls(rrs_vals,
                                              np.array(uncGrp.getDataset("ES_RADCAL_CAL").columns['1'], dtype=float)[ind_rad_wvl],
                                              waveSubset)

        ## Band Convolution of Uncertainties
        # get unc values at common wavebands (from ProcessL2) and convert any NaNs to 0 to not create issues with punpy
        esUNC_band = np.array([i[0] for i in xSlice['esUnc'].values()])
        liUNC_band = np.array([i[0] for i in xSlice['liUnc'].values()])
        ltUNC_band = np.array([i[0] for i in xSlice['ltUnc'].values()])
        esUNC_band[np.isnan(esUNC_band)] = 0.0
        liUNC_band[np.isnan(liUNC_band)] = 0.0
        ltUNC_band[np.isnan(ltUNC_band)] = 0.0
        ## Absolute uncertainties, after conversion from relative with field data, may have negative values
        # Take the absolute value of absolute uncertainties
        esUNC_band = np.abs(esUNC_band)
        liUNC_band = np.abs(liUNC_band)
        ltUNC_band = np.abs(ltUNC_band)

        if ConfigFile.settings["bL2WeightSentinel3A"]:

            # convolve instrument uncertainties to chosen bands also for later reporting alongside L2 products
            # it is more correct to convolve to L1B products before passing rrs through the measurement function to
            # acquire convolved Rrs Uncertainties.
            esDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['es'].values()), dtype=float).flatten(), waveSubset],
                [esUNC_band, None], "S3A")
            output["esUNC_Sentinel3A"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), esDeltaBand)}

            liDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['li'].values()), dtype=float).flatten(), waveSubset],
                [liUNC_band, None], "S3A")
            output["liUNC_Sentinel3A"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), liDeltaBand)}

            ltDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['lt'].values()), dtype=float).flatten(), waveSubset],
                [ltUNC_band, None], "S3A")
            output["ltUNC_Sentinel3A"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), ltDeltaBand)}

            rhoDeltaBand = Convolve.band_Conv_Uncertainty(
                [rho, waveSubset], [rhoUNC, None], "S3A")
            output["rhoUNC_Sentinel3A"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), rhoDeltaBand)}

            # it would be better to use the measurement function to acquire L2 uncertainties here, however a way should
            # be found that does not involve the convolution of all the measurement uncertainties. Essentially we do not
            # know the correlation between ES, LI, LT, & Rho which will create an overestimation of uncertainty.

            # TODO: explore using punpy to output the correlation between ES, LI, & LT to input into propagation of LW
            #  and Rrs

            output["lwUNC_Sentinel3A"] = Convolve.Propagate_Lw_Convolved(lw_means, lw_uncertainties,
                                                                         "S3A", waveSubset)
            output["rrsUNC_Sentinel3A"] = Convolve.Propagate_RRS_Convolved(rrs_means, rrs_uncertainties,
                                                                           "S3A", waveSubset)

        if ConfigFile.settings["bL2WeightSentinel3B"]:

            esDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['es'].values()), dtype=float).flatten(), waveSubset],
                [esUNC_band, None], "S3B")
            output["esUNC_Sentinel3B"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), esDeltaBand)}
            liDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['li'].values()), dtype=float).flatten(), waveSubset],
                [liUNC_band, None], "S3B")
            output["liUNC_Sentinel3B"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), liDeltaBand)}
            ltDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['lt'].values()), dtype=float).flatten(), waveSubset],
                [ltUNC_band, None], "S3B")
            output["ltUNC_Sentinel3B"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), ltDeltaBand)}
            rhoDeltaBand = Convolve.band_Conv_Uncertainty(
                [rho, waveSubset], [rhoUNC, None], "S3B")
            output["rhoUNC_Sentinel3B"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), rhoDeltaBand)}

            output["lwUNC_Sentinel3B"] = Convolve.Propagate_Lw_Convolved(lw_means, lw_uncertainties,
                                                                         "S3B", waveSubset)
            output["rrsUNC_Sentinel3B"] = Convolve.Propagate_RRS_Convolved(rrs_means, rrs_uncertainties,
                                                                           "S3B", waveSubset)
        if ConfigFile.settings['bL2WeightMODISA']:

            esDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['es'].values()), dtype=float).flatten(), waveSubset],
                [esUNC_band, None], "MOD-A")
            output["esUNC_MODISA"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), esDeltaBand)}

            liDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['li'].values()), dtype=float).flatten(), waveSubset],
                [liUNC_band, None], "MOD-A")
            output["liUNC_MODISA"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), liDeltaBand)}

            ltDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['lt'].values()), dtype=float).flatten(), waveSubset],
                [ltUNC_band, None], "MOD-A")
            output["ltUNC_MODISA"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), ltDeltaBand)}

            rhoDeltaBand = Convolve.band_Conv_Uncertainty([rho, waveSubset], [rhoUNC, None], "MOD-A")
            output["rhoUNC_MODISA"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), rhoDeltaBand)}

            output["lwUNC_MODISA"] = Convolve.Propagate_Lw_Convolved(lw_means, lw_uncertainties,
                                                                     "MOD-A", waveSubset)
            output["rrsUNC_MODISA"] = Convolve.Propagate_RRS_Convolved(rrs_means, rrs_uncertainties,
                                                                       "MOD-A", waveSubset)
        if ConfigFile.settings['bL2WeightMODIST']:

            esDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['es'].values()), dtype=float).flatten(), waveSubset],
                [esUNC_band, None], "MOD-T")
            output["esUNC_MODIST"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), esDeltaBand)}

            liDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['li'].values()), dtype=float).flatten(), waveSubset],
                [liUNC_band, None], "MOD-T")
            output["liUNC_MODIST"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), liDeltaBand)}

            ltDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['lt'].values()), dtype=float).flatten(), waveSubset],
                [ltUNC_band, None], "MOD-T")
            output["ltUNC_MODIST"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), ltDeltaBand)}

            rhoDeltaBand = Convolve.band_Conv_Uncertainty([rho, waveSubset], [rhoUNC, None], "MOD-T")
            output["rhoUNC_MODIST"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), rhoDeltaBand)}

            output["lwUNC_MODIST"] = Convolve.Propagate_Lw_Convolved(lw_means, lw_uncertainties,
                                                                     "MOD-T", waveSubset)
            output["rrsUNC_MODIST"] = Convolve.Propagate_RRS_Convolved(rrs_means, rrs_uncertainties,
                                                                       "MOD-T", waveSubset)
        if ConfigFile.settings['bL2WeightVIIRSN']:

            esDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['es'].values()), dtype=float).flatten(), waveSubset],
                [esUNC_band, None], "VIIRS-N")
            output["esUNC_VIIRSN"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), esDeltaBand)}

            liDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['li'].values()), dtype=float).flatten(), waveSubset],
                [liUNC_band, None], "VIIRS-N")
            output["liUNC_VIIRSN"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), liDeltaBand)}

            ltDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['lt'].values()), dtype=float).flatten(), waveSubset],
                [ltUNC_band, None], "VIIRS-N")
            output["ltUNC_VIIRSN"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), ltDeltaBand)}

            rhoDeltaBand = Convolve.band_Conv_Uncertainty([rho, waveSubset], [rhoUNC, None], "VIIRS-N")
            output["rhoUNC_VIIRSN"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), rhoDeltaBand)}

            output["lwUNC_VIIRSN"] = Convolve.Propagate_Lw_Convolved(lw_means, lw_uncertainties,
                                                                     "VIIRS-N", waveSubset)
            output["rrsUNC_VIIRSN"] = Convolve.Propagate_RRS_Convolved(rrs_means, rrs_uncertainties,
                                                                       "VIIRS-N", waveSubset)
        if ConfigFile.settings['bL2WeightVIIRSJ']:

            esDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['es'].values()), dtype=float).flatten(), waveSubset],
                [esUNC_band, None], "VIIRS-J")
            output["esUNC_VIIRSJ"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), esDeltaBand)}

            liDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['es'].values()), dtype=float).flatten(), waveSubset],
                [liUNC_band, None], "VIIRS-J")
            output["liUNC_VIIRSJ"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), liDeltaBand)}

            ltDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['es'].values()), dtype=float).flatten(), waveSubset],
                [ltUNC_band, None], "VIIRS-J")
            output["ltUNC_VIIRSJ"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), ltDeltaBand)}

            rhoDeltaBand = Convolve.band_Conv_Uncertainty([rho, waveSubset], [rhoUNC, None], "VIIRS-J")
            output["rhoUNC_VIIRSJ"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), rhoDeltaBand)}

            output["lwUNC_VIIRSJ"] = Convolve.Propagate_Lw_Convolved(lw_means, lw_uncertainties,
                                                                     "VIIRS-J", waveSubset)
            output["rrsUNC_VIIRSJ"] = Convolve.Propagate_RRS_Convolved(rrs_means, rrs_uncertainties,
                                                                       "VIIRS-J", waveSubset)
            pass
        output.update({"rhoUNC_HYPER": {str(k): val for k, val in zip(waveSubset, rhoUNC)},
                       "lwUNC": lwAbsUnc, "rrsUNC": rrsAbsUnc})

        return output

    def rrsHyperUNCFACTORY(self, node, uncGrp, rhoScalar, rhoVec, rhoDelta, waveSubset, xSlice):
        """

        :param node: HDFRoot which stores L1BQC data
        :param uncGrp: HDFGroup storing the uncertainty budget
        :param rhoScalar: rho input if Mobley99 or threeC rho is used
        :param rhoVec: rho input if Zhang17 rho is used
        :param rhoDelta: uncertainties associated with rho
        :param waveSubset: wavelength subset for any band convolution (and sizing rhoScalar if used)
        :param xSlice: Dictionary of input radiance, raw_counts, standard deviations etc.

        :return: dictionary of output uncertainties that are generated
        """

        waveSubset = np.array(waveSubset, dtype=float)  # convert waveSubset to numpy array
        esXstd = xSlice['esSTD_RAW']
        liXstd = xSlice['liSTD_RAW']
        ltXstd = xSlice['ltSTD_RAW']

        es, _ = self.interp_common_wvls(np.asarray(list(xSlice['es'].values()), dtype=float).flatten(),
                                       np.asarray(list(xSlice['es'].keys()), dtype=float).flatten(),
                                       np.array(uncGrp.getDataset("ES_RADCAL_UNC").columns['wvl'], dtype=float))
        li, _ = self.interp_common_wvls(np.asarray(list(xSlice['li'].values()), dtype=float).flatten(),
                                       np.asarray(list(xSlice['li'].keys()), dtype=float).flatten(),
                                       np.array(uncGrp.getDataset("LI_RADCAL_UNC").columns['wvl'], dtype=float))
        lt, _ = self.interp_common_wvls(np.asarray(list(xSlice['lt'].values()), dtype=float).flatten(),
                                       np.asarray(list(xSlice['lt'].keys()), dtype=float).flatten(),
                                       np.array(uncGrp.getDataset("LT_RADCAL_UNC").columns['wvl'], dtype=float))

        if rhoScalar is not None:  # make rho a constant array if scalar
            rho = np.ones(len(list(esXstd.keys())))*rhoScalar
            rhoUNC, _ = self.interp_common_wvls(np.array(rhoDelta, dtype=float),
                                                waveSubset,
                                                np.asarray(list(esXstd.keys()), dtype=float))
        else:  # zhang rho needs to be interpolated to radcal wavebands (len must be 255)
            rho, _ = self.interp_common_wvls(np.array(list(rhoVec.values()), dtype=float),
                                             waveSubset,
                                             np.asarray(list(esXstd.keys()), dtype=float))
            rhoUNC, _ = self.interp_common_wvls(rhoDelta,
                                                waveSubset,
                                                np.asarray(list(esXstd.keys()), dtype=float))

        # define dictionaries for uncertainty components
        Cal = {}
        Coeff = {}
        cPol = {}
        cStray = {}
        Ct = {}
        cLin = {}
        cStab = {}

        for sensor in ["ES", "LI", "LT"]:
            radcal = uncGrp.getDataset(f"{sensor}_RADCAL_UNC")
            radcal.datasetToColumns()

            cal_start = int(node.attributes['CAL_START'])
            cal_stop = int(node.attributes['CAL_STOP'])
            straylight = uncGrp.getDataset(f"{sensor}_STRAYDATA_CAL")
            straylight.datasetToColumns()
            cStray[sensor] = np.asarray(list(straylight.columns['1']))[cal_start:cal_stop + 1]
            # +1 here fixed a bug. Slicing arrays gives the first stop-start elements, not elements up to stop index.
            # Therefore for 255 pixels we need 0:255 not 0:254 to capture all pixels in the sl values.

            linear = uncGrp.getDataset(sensor + "_NLDATA_CAL")
            linear.datasetToColumns()
            cLin[sensor] = np.asarray(list(linear.columns['1']))

            stab = uncGrp.getDataset(sensor + "_STABDATA_CAL")
            stab.datasetToColumns()
            cStab[sensor] = np.asarray(list(stab.columns['1']))

            ### ADERU : Read coeff value from configuration files
            Cal[sensor] = np.asarray(list(radcal.columns['unc']))
            calFolder = os.path.splitext(ConfigFile.filename)[0] + "_Calibration"
            calPath = os.path.join(PATH_TO_CONFIG, calFolder)
            calibrationMap = CalibrationFileReader.read(calPath)
            waves, Coeff[sensor] = ProcessL1b_FactoryCal.extract_calibration_coeff(node, calibrationMap, sensor)

            pol = uncGrp.getDataset(sensor + "_POLDATA_CAL")
            pol.datasetToColumns()
            cPol[sensor] = np.asarray(list(pol.columns['1']))  # es cos stored in poldata file as temporary fix

            # temp uncertainties calculated at L1AQC
            Temp = uncGrp.getDataset(sensor + "_TEMPDATA_CAL")
            Temp.datasetToColumns()
            Ct[sensor] = np.array(Temp.columns[f'{sensor}_TEMPERATURE_UNCERTAINTIES'])

        Propagate_L2 = Propagate(M=100, cores=0)
        slice_size = len(es)
        ones = np.ones(slice_size)

        lw_means = [lt, rho, li,
                   ones, ones,  # Coeff['LI'], Coeff['LT'],
                   ones, ones,
                   ones, ones,
                   ones, ones,
                   ones, ones,
                   ones, ones]

        lw_uncertainties = [np.abs(np.array(list(ltXstd.values())).flatten() * lt),
                           rhoUNC,
                           np.abs(np.array(list(liXstd.values())).flatten() * li),
                           Cal['LI']/200, Cal['LT']/200,
                           cStab['LI'], cStab['LT'],
                           cLin['LI'], cLin['LT'],
                           cStray['LI']/100, cStray['LI']/100,
                           Ct['LI'], Ct['LI'],
                           cPol['LI'], cPol['LI']]

        # NOTE: ISSUE #95
        lwAbsUnc = Propagate_L2.Propagate_Lw_HYPER(lw_means, lw_uncertainties)
        lw_vals = Propagate_L2.Lw(*lw_means)

        rrs_means = [lt, rho, li, es,
                ones, ones, ones,
                ones, ones, ones,
                ones, ones, ones,
                ones, ones, ones,
                ones, ones, ones,
                ones, ones, ones]

        rrs_uncertainties = [np.abs(np.array(list(ltXstd.values())).flatten() * lt),
                             rhoUNC,
                             np.abs(np.array(list(liXstd.values())).flatten() * li),
                             np.abs(np.array(list(esXstd.values())).flatten() * es),
                             Cal['ES']/200, Cal['LI']/200, Cal['LT']/200,
                             cStab['ES'], cStab['LI'], cStab['LT'],
                             cLin['ES'], cLin['LI'], cLin['LT'],
                             cStray['ES']/100, cStray['LI']/100, cStray['LT']/100,
                             Ct['ES'], Ct['LI'], Ct['LT'],
                             cPol['LI'], cPol['LT'], cPol['ES']
                             ]

        rrsAbsUnc = Propagate_L2.Propagate_RRS_HYPER(rrs_means, rrs_uncertainties)
        rrs_vals = Propagate_L2.RRS(*rrs_means)

        ## BAND CONVOLUTION
        # band convolution of uncertainties is done here to include uncertainty contribution of band convolution process
        Convolve = Propagate(M=100, cores=1)
        # these are absolute values! Dont get confused

        output = {}  # create dictionary to store uncertainty values which are returned from methods

        # interpolate output uncertainties to the waveSubset (common wavebands of interpolated es, li, & lt)
        # wvls = np.asarray(list(xSlice['es'].keys()), dtype=float)
        lwAbsUnc, _ = self.interp_common_wvls(lwAbsUnc,
                                             np.array(uncGrp.getDataset("ES_RADCAL_UNC").columns['wvl'], dtype=float),
                                             waveSubset)
        lw_vals, _ = self.interp_common_wvls(lw_vals,
                                            np.array(uncGrp.getDataset("ES_RADCAL_UNC").columns['wvl'], dtype=float),
                                            waveSubset)
        rrsAbsUnc, _ = self.interp_common_wvls(rrsAbsUnc,
                                              np.array(uncGrp.getDataset("ES_RADCAL_UNC").columns['wvl'], dtype=float),
                                              waveSubset)
        rrs_vals, _ = self.interp_common_wvls(rrs_vals,
                                             np.array(uncGrp.getDataset("ES_RADCAL_UNC").columns['wvl'], dtype=float),
                                             waveSubset)

        # Plot Class based L2 uncertainties
        if ConfigFile.settings['bL2UncertaintyBreakdownPlot']:
            # if I understand how the stations group in the L1BQC hdf works then this should always be a list with one
            # value
            stations = np.array(node.getGroup("ANCILLARY").getDataset("STATION").columns["STATION"])
            if stations is not None:
                cast = (f"{type(self).__name__}_{node.attributes['CAST']}_"
                        f"{np.unique(stations[~np.isnan(stations)]).tolist()[0]}")
            else:
                cast = f"{type(self).__name__}_{node.attributes['CAST']}"

            p_unc = UncertaintyGUI()
            p_unc.pie_plot_class_l2(
                rrs_means,
                lw_means,
                rrs_uncertainties,
                lw_uncertainties,
                np.array(waves),  # pass radcal wavelengths
                cast,
                node.getGroup("ANCILLARY")
            )
            p_unc.plot_class_L2(
                rrs_means,
                lw_means,
                rrs_uncertainties,
                lw_uncertainties,
                np.array(waves),
                cast
            )

        ## Band Convolution of Uncertainties
        # get unc values at common wavebands (from ProcessL2) and convert any NaNs to 0 to not create issues with punpy
        esUNC_band = np.array([i[0] for i in xSlice['esUnc'].values()])
        liUNC_band = np.array([i[0] for i in xSlice['liUnc'].values()])
        ltUNC_band = np.array([i[0] for i in xSlice['ltUnc'].values()])
        esUNC_band[np.isnan(esUNC_band)] = 0.0
        liUNC_band[np.isnan(liUNC_band)] = 0.0
        ltUNC_band[np.isnan(ltUNC_band)] = 0.0
        ## Absolute uncertainties, after conversion from relative with field data, may have negative values
        # Take the absolute value of absolute uncertainties
        esUNC_band = np.abs(esUNC_band)
        liUNC_band = np.abs(liUNC_band)
        ltUNC_band = np.abs(ltUNC_band)

        if ConfigFile.settings["bL2WeightSentinel3A"]:
            esDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['es'].values()), dtype=float).flatten(), waveSubset],
                [esUNC_band, None], "S3A")
            output["esUNC_Sentinel3A"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), esDeltaBand)}

            liDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['li'].values()), dtype=float).flatten(), waveSubset],
                [liUNC_band, None], "S3A")
            output["liUNC_Sentinel3A"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), liDeltaBand)}

            ltDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['lt'].values()), dtype=float).flatten(), waveSubset],
                [ltUNC_band, None], "S3A")
            output["ltUNC_Sentinel3A"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), ltDeltaBand)}

            rhoDeltaBand = Convolve.band_Conv_Uncertainty(
                [rho, waveSubset], [rhoUNC, None], "S3A")
            output["rhoUNC_Sentinel3A"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), rhoDeltaBand)}

            output["lwUNC_Sentinel3A"] = Convolve.Propagate_Lw_Convolved(lw_means, lw_uncertainties,
                                                                         "S3A", waveSubset)
            output["rrsUNC_Sentinel3A"] = Convolve.Propagate_RRS_Convolved(rrs_means, rrs_uncertainties,
                                                                           "S3A", waveSubset)

        if ConfigFile.settings["bL2WeightSentinel3B"]:

            esDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['es'].values()), dtype=float).flatten(), waveSubset],
                [esUNC_band, None], "S3B")
            output["esUNC_Sentinel3B"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), esDeltaBand)}
            liDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['li'].values()), dtype=float).flatten(), waveSubset],
                [liUNC_band, None], "S3B")
            output["liUNC_Sentinel3B"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), liDeltaBand)}
            ltDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['lt'].values()), dtype=float).flatten(), waveSubset],
                [ltUNC_band, None], "S3B")
            output["ltUNC_Sentinel3B"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), ltDeltaBand)}
            rhoDeltaBand = Convolve.band_Conv_Uncertainty(
                [rho, waveSubset], [rhoUNC, None], "S3B")
            output["rhoUNC_Sentinel3B"] = {str(k): [val] for k, val in zip(Weight_RSR.Sentinel3Bands(), rhoDeltaBand)}

            output["lwUNC_Sentinel3B"] = Convolve.Propagate_Lw_Convolved(lw_means, lw_uncertainties,
                                                                         "S3B", waveSubset)
            output["rrsUNC_Sentinel3B"] = Convolve.Propagate_RRS_Convolved(rrs_means, rrs_uncertainties,
                                                                           "S3B", waveSubset)
        if ConfigFile.settings['bL2WeightMODISA']:

            esDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['es'].values()), dtype=float).flatten(), waveSubset],
                [esUNC_band, None], "MOD-A")
            output["esUNC_MODISA"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), esDeltaBand)}

            liDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['li'].values()), dtype=float).flatten(), waveSubset],
                [liUNC_band, None], "MOD-A")
            output["liUNC_MODISA"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), liDeltaBand)}

            ltDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['lt'].values()), dtype=float).flatten(), waveSubset],
                [ltUNC_band, None], "MOD-A")
            output["ltUNC_MODISA"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), ltDeltaBand)}
            rhoDeltaBand = Convolve.band_Conv_Uncertainty([rho, waveSubset], [rhoUNC, None], "MOD-A")

            output["rhoUNC_MODISA"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), rhoDeltaBand)}

            output["lwUNC_MODISA"] = Convolve.Propagate_Lw_Convolved(lw_means, lw_uncertainties,
                                                                     "MOD-A", waveSubset)
            output["rrsUNC_MODISA"] = Convolve.Propagate_RRS_Convolved(rrs_means, rrs_uncertainties,
                                                                       "MOD-A", waveSubset)
        if ConfigFile.settings['bL2WeightMODIST']:

            esDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['es'].values()), dtype=float).flatten(), waveSubset],
                [esUNC_band, None], "MOD-T")
            output["esUNC_MODIST"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), esDeltaBand)}

            liDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['li'].values()), dtype=float).flatten(), waveSubset],
                [liUNC_band, None], "MOD-T")
            output["liUNC_MODIST"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), liDeltaBand)}

            ltDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['lt'].values()), dtype=float).flatten(), waveSubset],
                [ltUNC_band, None], "MOD-T")
            output["ltUNC_MODIST"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), ltDeltaBand)}

            rhoDeltaBand = Convolve.band_Conv_Uncertainty([rho, waveSubset], [rhoUNC, None], "MOD-T")

            output["rhoUNC_MODIST"] = {str(k): [val] for k, val in zip(Weight_RSR.MODISBands(), rhoDeltaBand)}
            output["lwUNC_MODIST"] = Convolve.Propagate_Lw_Convolved(lw_means, lw_uncertainties,
                                                                     "MOD-T", waveSubset)
            output["rrsUNC_MODIST"] = Convolve.Propagate_RRS_Convolved(rrs_means, rrs_uncertainties,
                                                                       "MOD-T", waveSubset)
        if ConfigFile.settings['bL2WeightVIIRSN']:

            esDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['es'].values()), dtype=float).flatten(), waveSubset],
                [esUNC_band, None], "VIIRS-N")
            output["esUNC_VIIRSN"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), esDeltaBand)}

            liDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['li'].values()), dtype=float).flatten(), waveSubset],
                [liUNC_band, None], "VIIRS-N")
            output["liUNC_VIIRSN"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), liDeltaBand)}

            ltDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['lt'].values()), dtype=float).flatten(), waveSubset],
                [ltUNC_band, None], "VIIRS-N")
            output["ltUNC_VIIRSN"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), ltDeltaBand)}

            rhoDeltaBand = Convolve.band_Conv_Uncertainty([rho, waveSubset], [rhoUNC, None], "VIIRS-N")

            output["rhoUNC_VIIRSN"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), rhoDeltaBand)}
            output["lwUNC_VIIRSN"] = Convolve.Propagate_Lw_Convolved(lw_means, lw_uncertainties,
                                                                     "VIIRS-N", waveSubset)
            output["rrsUNC_VIIRSN"] = Convolve.Propagate_RRS_Convolved(rrs_means, rrs_uncertainties,
                                                                       "VIIRS-N", waveSubset)
        if ConfigFile.settings['bL2WeightVIIRSJ']:

            esDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['es'].values()), dtype=float).flatten(), waveSubset],
                [esUNC_band, None], "VIIRS-J")
            output["esUNC_VIIRSJ"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), esDeltaBand)}

            liDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['es'].values()), dtype=float).flatten(), waveSubset],
                [liUNC_band, None], "VIIRS-J")
            output["liUNC_VIIRSJ"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), liDeltaBand)}

            ltDeltaBand = Convolve.band_Conv_Uncertainty(
                [np.asarray(list(xSlice['es'].values()), dtype=float).flatten(), waveSubset],
                [ltUNC_band, None], "VIIRS-J")
            output["ltUNC_VIIRSJ"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), ltDeltaBand)}

            rhoDeltaBand = Convolve.band_Conv_Uncertainty([rho, waveSubset], [rhoUNC, None], "VIIRS-J")

            output["rhoUNC_VIIRSJ"] = {str(k): [val] for k, val in zip(Weight_RSR.VIIRSBands(), rhoDeltaBand)}
            output["lwUNC_VIIRSJ"] = Convolve.Propagate_Lw_Convolved(lw_means, lw_uncertainties,
                                                                     "VIIRS-J", waveSubset)
            output["rrsUNC_VIIRSJ"] = Convolve.Propagate_RRS_Convolved(rrs_means, rrs_uncertainties,
                                                                       "VIIRS-J", waveSubset)
            pass
        output.update({"rhoUNC_HYPER": {str(k): val for k, val in zip(waveSubset, rhoUNC)},
                       "lwUNC": lwAbsUnc, "rrsUNC": rrsAbsUnc})

        return output

    ## Utilties
    @staticmethod
    def interp_common_wvls(columns, waves, newWavebands):
        saveTimetag2 = None
        if isinstance(columns, dict):
            if "Datetag" in columns:
                saveDatetag = columns.pop("Datetag")
                saveTimetag2 = columns.pop("Timetag2")
                columns.pop("Datetime")
            y = np.asarray(list(columns.values()))
        elif isinstance(columns, np.ndarray):  # is numpy array
            y = columns
        else:
            msg = "columns are unexpected type: ProcessInstrumentUncertainties.py - interp_common_wvls"
            print(msg)
        # Get wavelength values
        x = np.asarray(waves)

        newColumns = collections.OrderedDict()
        if saveTimetag2 is not None:
            newColumns["Datetag"] = saveDatetag
            newColumns["Timetag2"] = saveTimetag2
        # Can leave Datetime off at this point

        for i in range(newWavebands.shape[0]):
            newColumns[str(round(10*newWavebands[i])/10)] = []  # limit to one decimal place

        new_y = np.interp(newWavebands, x, y)  #InterpolatedUnivariateSpline(x, y, k=3)(newWavebands)

        for waveIndex in range(newWavebands.shape[0]):
            newColumns[str(round(10*newWavebands[waveIndex])/10)].append(new_y[waveIndex])

        return new_y, newColumns

    @staticmethod
    def interpolateSamples(Columns, waves, newWavebands):
        '''
        Wavelength Interpolation for differently sized arrays containing samples
        Use a common waveband set determined by the maximum lowest wavelength
        of all sensors, the minimum highest wavelength, and the interval
        set in the Configuration Window.
        '''

        # Copy dataset to dictionary
        columns = {k: Columns[:, i] for i, k in enumerate(waves)}
        cols = []
        for m in range(Columns.shape[0]):  # across all the monte carlo draws
            newColumns = {}

            for i in range(newWavebands.shape[0]):
                # limit to one decimal place
                newColumns[str(round(10*newWavebands[i])/10)] = []

            # for m in range(Columns.shape[0]):
            # Perform interpolation for each timestamp
            y = np.asarray([columns[k][m] for k in columns])

            new_y = sp.interpolate.InterpolatedUnivariateSpline(waves, y, k=3)(newWavebands)

            for waveIndex in range(newWavebands.shape[0]):
                newColumns[str(round(10*newWavebands[waveIndex])/10)].append(new_y[waveIndex])

            cols.append(newColumns)

        return np.asarray(cols)

    def gen_n_IB_sample(self, mDraws):
        # make your own sample here min is 3, max is 6 - all values must be integer
        import random as rand
        # seed random number generator with current systime (default behaviour of rand.seed)
        rand.seed(a=None, version=2)
        sample_n_IB = []
        for i in range(mDraws):
            sample_n_IB.append(rand.randrange(3, 7, 1))  # sample_n_IB max should be 6
        return np.asarray(sample_n_IB)  # make numpy array to be compatible with comet maths

    def get_Slaper_Sl_unc(self, data, sample_data, mZ, sample_mZ, n_iter, sample_n_iter, MC_prop, mDraws):
        """
        finds the uncertainty in the slaper correction. Error estimated from the difference between slaper correction
        using n_iter and n_iter - 1

        :param data: signal to be corrected (either S12 or signal)
        :param sample_data: MC sample [PDF] of data attribute
        :param mZ: LSF read from tartu files
        :param sample mZ: PDF of mZ
        :param n_iter: number of iterations
        :param sample_n_iter: simple PDF of n_iter, no uncertainty should be passed here.
        :param MC_prop: punpy.MCP object as namespace for calling punpy functions/settings
        :param mDraws: number of monte carlo draws, M
        """
        # calculates difference between n=4 and n=5, then propagates as an error
        sl_corr = self.Slaper_SL_correction(data, mZ, n_iter)
        sl_corr_unc = []
        sl4 = self.Slaper_SL_correction(data, mZ, n_iter=n_iter - 1)
        for i in range(len(sl_corr)):  # get the difference between n=4 and n=5
            if sl_corr[i] > sl4[i]:
                sl_corr_unc.append(sl_corr[i] - sl4[i])
            else:
                sl_corr_unc.append(sl4[i] - sl_corr[i])

        sample_sl_syst = cm.generate_sample(mDraws, sl_corr, np.array(sl_corr_unc), "syst")
        sample_sl_rand = MC_prop.run_samples(self.Slaper_SL_correction, [sample_data, sample_mZ, sample_n_iter])
        sample_sl_corr = MC_prop.combine_samples([sample_sl_syst, sample_sl_rand])

        return sample_sl_corr

    # Measurement Functions
    @staticmethod
    def S12func(k, S1, S2):
        return ((1 + k)*S1) - (k*S2)

    @staticmethod
    def alphafunc(S1, S12):
        t1 = [Decimal(S1[i]) - Decimal(S12[i]) for i in range(len(S1))]
        t2 = [pow(Decimal(S12[i]), 2) for i in range(len(S12))]
        return np.asarray([float(t1[i]/t2[i]) for i in range(len(t1))])

    @staticmethod
    def dark_Substitution(light, dark):
        return light - dark

    @staticmethod
    def non_linearity_corr(offset_corrected_mesure, alpha):
        linear_corr_mesure = offset_corrected_mesure*(1 - alpha*offset_corrected_mesure)
        return linear_corr_mesure

    @staticmethod
    def Zong_SL_correction(input_data, C_matrix):
        return np.matmul(C_matrix, input_data)

    @staticmethod
    def Slaper_SL_correction(input_data, SL_matrix, n_iter=5):
        nband = len(input_data)
        m_norm = np.zeros(nband)

        mC = np.zeros((n_iter + 1, nband))
        mX = np.zeros((n_iter + 1, nband))
        mZ = SL_matrix
        mX[0, :] = input_data

        for i in range(nband):
            jstart = np.max([0, i - 10])
            jstop = np.min([nband, i + 10])
            m_norm[i] = np.sum(mZ[i, jstart:jstop])  # eq 4

        for i in range(nband):
            if m_norm[i] == 0:
                mZ[i, :] = np.zeros(nband)
            else:
                mZ[i, :] = mZ[i, :]/m_norm[i]  # eq 5

        for k in range(1, n_iter + 1):
            for i in range(nband):
                mC[k - 1, i] = mC[k - 1, i] + np.sum(mX[k - 1, :]*mZ[i, :])  # eq 6
                if mC[k - 1, i] == 0:
                    mX[k, i] = 0
                else:
                    mX[k, i] = (mX[k - 1, i]*mX[0, i])/mC[k - 1, i]  # eq 7

        return mX[n_iter - 1, :]

    @staticmethod
    def absolute_calibration(normalized_mesure, updated_radcal_gain):
        return normalized_mesure/updated_radcal_gain

    @staticmethod
    def thermal_corr(Ct, calibrated_mesure):
        return Ct*calibrated_mesure

    @staticmethod
    def prepare_cos(uncGrp, sensortype, level=None, ind_raw_wvl=None):
        """
        read from hdf and prepare inputs for cos_err measurement function
        """
        ## Angular cosine correction (for Irradiance)
        if level != 'L2':
            radcal_wvl = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['1'][1:].tolist())
            coserror = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR").data))[1:, 2:]
            cos_unc = (np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_UNCERTAINTY").data))[1:, 2:]
                       /100)*np.abs(coserror)

            coserror_90 = np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR_AZ90").data))[1:, 2:]
            cos90_unc = (np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_UNCERTAINTY_AZ90").data))[1:,
                         2:]/100)*np.abs(coserror_90)
        else:
            # reading in data changes if at L2 (because hdf files have different layout)
            radcal_wvl = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['1'][1:].tolist())
            coserror = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR").data))[1:, 2:]
            cos_unc = (np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_UNCERTAINTY").data))[1:, 2:]/100)*np.abs(coserror)
            coserror_90 = np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR_AZ90").data))[1:, 2:]
            cos90_unc = (np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_UNCERTAINTY_AZ90").data))[1:, 2:]/100)*np.abs(coserror_90)

        radcal_unc = None  # no uncertainty in the wavelengths as they are only used to index

        zenith_ang = uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR").attributes["COLUMN_NAMES"].split('\t')[2:]
        zenith_ang = np.asarray([float(x) for x in zenith_ang])
        zen_unc = np.asarray([0.05 for x in zenith_ang])  # default of 0.5 for solar zenith unc

        if ind_raw_wvl is not None:
            radcal_wvl = radcal_wvl[ind_raw_wvl]
            coserror = coserror[ind_raw_wvl]
            coserror_90 = coserror_90[ind_raw_wvl]
            cos_unc = cos_unc[ind_raw_wvl]
            cos90_unc = cos90_unc[ind_raw_wvl]

        return [radcal_wvl, coserror, coserror_90, zenith_ang], [radcal_unc, cos_unc, cos90_unc, zen_unc]

    @staticmethod
    def AZAvg_Coserr(coserror, coserror_90):
        # if delta < 2% : averaging the 2 azimuth plan
        return (coserror + coserror_90)/2.  # average azi coserr

    @staticmethod
    def ZENAvg_Coserr(radcal_wvl, AZI_avg_coserror):
        i1 = np.argmin(np.abs(radcal_wvl - 300))
        i2 = np.argmin(np.abs(radcal_wvl - 1000))

        # if delta < 2% : averaging symetric zenith
        ZEN_avg_coserror = (AZI_avg_coserror + AZI_avg_coserror[:, ::-1])/2.

        # set coserror to 1 outside range [450,700]
        ZEN_avg_coserror[0:i1, :] = 0
        ZEN_avg_coserror[i2:, :] = 0
        return ZEN_avg_coserror

    @staticmethod
    def FHemi_Coserr(ZEN_avg_coserror, zenith_ang):
        # Compute full hemisperical coserror
        zen0 = np.argmin(np.abs(zenith_ang))
        zen90 = np.argmin(np.abs(zenith_ang - 90))
        deltaZen = (zenith_ang[1::] - zenith_ang[:-1])

        full_hemi_coserror = np.zeros(ZEN_avg_coserror.shape[0])

        for i in range(ZEN_avg_coserror.shape[0]):
            full_hemi_coserror[i] = np.sum(
                ZEN_avg_coserror[i, zen0:zen90]*np.sin(2*np.pi*zenith_ang[zen0:zen90]/180)*deltaZen[
                                                                                           zen0:zen90]*np.pi/180)

        return full_hemi_coserror

    @staticmethod
    def cosine_corr(avg_coserror, full_hemi_coserror, zenith_ang, thermal_corr_mesure, sol_zen, dir_rat):
        ind_closest_zen = np.argmin(np.abs(zenith_ang - sol_zen))
        cos_corr = 1 - avg_coserror[:, ind_closest_zen]/100
        Fhcorr = 1 - np.array(full_hemi_coserror)/100
        cos_corr_mesure = (dir_rat*thermal_corr_mesure*cos_corr) + ((1 - dir_rat)*thermal_corr_mesure*Fhcorr)

        return cos_corr_mesure

    @staticmethod
    def get_cos_corr(zenith_angle, solar_zenith, cosine_error):
        ind_closest_zen = np.argmin(np.abs(zenith_angle - solar_zenith))
        return 1 - cosine_error[:, ind_closest_zen]/100

    @staticmethod
    def cos_corr(signal, direct_ratio, cos_correction, full_hemi_cos_error):
        Fhcorr = (1 - full_hemi_cos_error / 100)
        return (direct_ratio * signal * cos_correction) + ((1 - direct_ratio) * signal * Fhcorr)

    @staticmethod
    def cos_corr_fun(avg_coserror, zenith_ang, sol_zen):
        ind_closest_zen = np.argmin(np.abs(zenith_ang - sol_zen))
        return 1 - avg_coserror[:, ind_closest_zen]/100

    @staticmethod
    def cosine_error_correction(uncGrp, sensortype):

        ## Angular cosine correction (for Irradiance)
        radcal_wvl = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['1'][1:].tolist())
        coserror = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR").data))[1:,2:]
        coserror_90 = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR_AZ90").data))[1:, 2:]
        coserror_unc = (np.asarray(
            pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_UNCERTAINTY").data))[1:,2:]/100)*coserror
        coserror_90_unc = (np.asarray(
            pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_UNCERTAINTY_AZ90").data))[1:, 2:]/100)*coserror_90
        zenith_ang = uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR").attributes["COLUMN_NAMES"].split('\t')[2:]
        i1 = np.argmin(np.abs(radcal_wvl - 300))
        i2 = np.argmin(np.abs(radcal_wvl - 1000))
        zenith_ang = np.asarray([float(x) for x in zenith_ang])

        # comparing cos_error for 2 azimuth
        AZI_delta_err = np.abs(coserror - coserror_90)

        # if delta < 2% : averaging the 2 azimuth plan
        AZI_avg_coserror = (coserror + coserror_90)/2.
        AZI_delta = np.power(np.power(coserror_unc, 2) + np.power(coserror_90_unc, 2), 0.5)  # TODO: check this!

        # comparing cos_error for symetric zenith
        ZEN_delta_err = np.abs(AZI_avg_coserror - AZI_avg_coserror[:, ::-1])
        ZEN_delta = np.power(np.power(AZI_delta, 2) + np.power(AZI_delta[:, ::-1], 2), 0.5)

        # if delta < 2% : averaging symetric zenith
        ZEN_avg_coserror = (AZI_avg_coserror + AZI_avg_coserror[:, ::-1])/2.

        # set coserror to 1 outside range [450,700]
        ZEN_avg_coserror[0:i1, :] = 0
        ZEN_avg_coserror[i2:, :] = 0

        return ZEN_avg_coserror, AZI_avg_coserror, zenith_ang, ZEN_delta_err, ZEN_delta, AZI_delta_err, AZI_delta


    @staticmethod
    def read_py6s_model(node):
        res_py6s = {}
        
        # Create a temporary group to pop date time columns
        newGrp = node.addGroup('temp')
        newGrp.copy(node.getGroup('PY6S_MODEL'))
        for ds in newGrp.datasets:
            newGrp.datasets[ds].datasetToColumns()
        py6s_gp = node.getGroup('temp')
        
        py6s_gp.getDataset("direct_ratio").columns.pop('Datetime')
        py6s_gp.getDataset("direct_ratio").columns.pop('Timetag2')
        py6s_gp.getDataset("direct_ratio").columns.pop('Datetag')
        py6s_gp.getDataset("direct_ratio").columnsToDataset()
        py6s_gp.getDataset("diffuse_ratio").columns.pop('Datetime')
        py6s_gp.getDataset("diffuse_ratio").columns.pop('Timetag2')
        py6s_gp.getDataset("diffuse_ratio").columns.pop('Datetag')
        py6s_gp.getDataset("diffuse_ratio").columnsToDataset()

        # py6s_gp.getDataset("direct_ratio").datasetToColumns()
        res_py6s['solar_zenith'] = np.asarray(py6s_gp.getDataset('solar_zenith').columns['solar_zenith'])
        # changed to 3: because timetag2 was included and causing a bug
        res_py6s['wavelengths'] = np.asarray(list(py6s_gp.getDataset('direct_ratio').columns.keys())[3:], dtype=float)
        res_py6s['direct_ratio'] = np.asarray(pd.DataFrame(py6s_gp.getDataset("direct_ratio").data))
        res_py6s['diffuse_ratio'] = np.asarray(pd.DataFrame(py6s_gp.getDataset("diffuse_ratio").data))
        node.removeGroup(py6s_gp)
        return res_py6s



class HyperOCR(Instrument):

    warnings.filterwarnings("ignore", message="One of the provided covariance matrix is not positivedefinite. It has been slightly changed")

    def __init__(self):
        super().__init__()  # call to instrument __init__
        self.instrument = "HyperOCR"

    @staticmethod
    def _check_data(dark, light):
        msg = None
        if (dark is None) or (light is None):
            msg = f'Dark Correction, dataset not found: {dark} , {light}'
            print(msg)
            Utilities.writeLogFile(msg)
            return False

        if Utilities.hasNan(light):
            frameinfo = getframeinfo(currentframe())
            msg = f'found NaN {frameinfo.lineno}'

        if Utilities.hasNan(dark):
            frameinfo = getframeinfo(currentframe())
            msg = f'found NaN {frameinfo.lineno}'
        if msg:
            print(msg)
            Utilities.writeLogFile(msg)
        return True

    def darkToLightTimer(self, rawGrp, sensortype):
        darkGrp = rawGrp['DARK']
        lightGrp = rawGrp['LIGHT']

        if darkGrp.attributes["FrameType"] == "ShutterDark" and darkGrp.getDataset(sensortype):
            darkData = darkGrp.getDataset(sensortype)
            darkDateTime = darkGrp.getDataset("DATETIME")
        if lightGrp.attributes["FrameType"] == "ShutterLight" and lightGrp.getDataset(sensortype):
            lightData = lightGrp.getDataset(sensortype)
            lightDateTime = lightGrp.getDataset("DATETIME")

        if darkGrp is None or lightGrp is None:
            msg = f'No radiometry found for {sensortype}'
            print(msg)
            Utilities.writeLogFile(msg)
            return False
        elif not self._check_data(darkData, lightData):
            return False

        newDarkData = self._interp(lightData, lightDateTime, darkData, darkDateTime)
        if isinstance(newDarkData, bool):
            return False
        else:
            rawGrp['DARK'].datasets[sensortype].data = newDarkData
            rawGrp['DARK'].datasets[sensortype].datasetToColumns()
            return True

    @staticmethod
    def _interp(lightData, lightTimer, darkData, darkTimer):
        # Interpolate Dark Dataset to match number of elements as Light Dataset
        newDarkData = np.copy(lightData.data)
        for k in darkData.data.dtype.fields.keys():  # darkData.data.dtype.fields.keys():  # For each wavelength
            x = np.copy(darkTimer.data).tolist()  # darktimer
            y = np.copy(darkData.data[k]).tolist()  # data at that band over time
            new_x = lightTimer.data  # lighttimer

            if len(x) < 3 or len(y) < 3 or len(new_x) < 3:
                msg = "**************Cannot do cubic spline interpolation, length of datasets < 3"
                print(msg)
                Utilities.writeLogFile(msg)
                return False
            if not Utilities.isIncreasing(x):
                msg = "**************darkTimer does not contain strictly increasing values"
                print(msg)
                Utilities.writeLogFile(msg)
                return False
            if not Utilities.isIncreasing(new_x):
                msg = "**************lightTimer does not contain strictly increasing values"
                print(msg)
                Utilities.writeLogFile(msg)
                return False

            if len(x) >= 3:
                # Because x is now a list of datetime tuples, they'll need to be
                # converted to Unix timestamp values
                xTS = [calendar.timegm(xDT.utctimetuple()) + xDT.microsecond / 1E6 for xDT in x]
                newXTS = [calendar.timegm(xDT.utctimetuple()) + xDT.microsecond / 1E6 for xDT in new_x]

                newDarkData[k] = Utilities.interp(xTS,y,newXTS, fill_value=np.nan)

                for val in newDarkData[k]:
                    if np.isnan(val):
                        frameinfo = getframeinfo(currentframe())
                        msg = f'found NaN {frameinfo.lineno}'
            else:
                msg = '**************Record too small for splining. Exiting.'
                print(msg)
                Utilities.writeLogFile(msg)
                return False

        if Utilities.hasNan(darkData):
            frameinfo = getframeinfo(currentframe())
            msg = f'found NaN {frameinfo.lineno}'
            print(msg)
            Utilities.writeLogFile(msg)
            exit()

        return newDarkData

    def lightDarkStats(self, grp, slice, sensortype):
        # SeaBird HyperOCR
        lightGrp = grp[0]
        lightSlice = copy.deepcopy(slice[0])  # copy to prevent changing of Raw data
        darkGrp = grp[1]
        darkSlice = copy.deepcopy(slice[1])

        if darkGrp.attributes["FrameType"] == "ShutterDark" and darkGrp.getDataset(sensortype):
            darkData = darkSlice['data']  # darkGrp.getDataset(sensortype)
            # darkDateTime = darkSlice['datetime']  # darkGrp.getDataset("DATETIME")

        if lightGrp.attributes["FrameType"] == "ShutterLight" and lightGrp.getDataset(sensortype):
            lightData = lightSlice['data']  # lightGrp.getDataset(sensortype)
            # lightDateTime = lightSlice['datetime']  # lightGrp.getDataset("DATETIME")

        if darkGrp is None or lightGrp is None:
            msg = f'No radiometry found for {sensortype}'
            print(msg)
            Utilities.writeLogFile(msg)
            return False

        elif not self._check_data(darkData, lightData):
            return False
        # Do interpolation at the start of the stations ensemble process, then slice like light data
        # newDarkData = self._interp(lightData, lightDateTime, darkData, darkDateTime)
        # if not newDarkData:
        #     return False

        # Correct light data by subtracting interpolated dark data from light data
        std_Light = []
        std_Dark = []
        ave_Light = []
        ave_Dark = []
        stdevSignal = {}

        # number of replicates for light and dark readings
        N = np.asarray(list(lightData.values())).shape[1]
        Nd = np.asarray(list(darkData.values())).shape[1]

        for i, k in enumerate(lightData.keys()):
            wvl = str(float(k))

            # apply normalisation to the standard deviations used in uncertainty calculations
            if N > 25:  # normal case
                std_Light.append(np.std(lightData[k])/np.sqrt(N))
                std_Dark.append(np.std(darkData[k])/np.sqrt(Nd) )  # sigma here is essentially sigma**2 so N must sqrt
            else:  # few scans, use different statistics
                std_Light.append((N-1/N-3)*(lightData[k] / np.sqrt(N))**2)
                std_Dark.append((Nd-1/Nd-3)*(darkData[k] / np.sqrt(Nd))**2)

            ave_Light.append(np.average(lightData[k]))
            ave_Dark.append(np.average(darkData[k]))

            for x in range(N):
                lightData[k][x] -= darkData[k][x]

            signalAve = np.average(lightData[k])

            # Normalised signal standard deviation =
            if signalAve:
                stdevSignal[wvl] = pow((pow(std_Light[i], 2) + pow(std_Dark[i], 2))/pow(signalAve, 2), 0.5)
            else:
                stdevSignal[wvl] = 0.0

        return dict(
            ave_Light=np.array(ave_Light),
            ave_Dark=np.array(ave_Dark),
            std_Light=np.array(std_Light),
            std_Dark=np.array(std_Dark),
            std_Signal=stdevSignal,
            )

    def FRM(self, node, uncGrp, raw_grps, raw_slices, stats, newWaveBands):
        # calibration of HyperOCR following the FRM processing of FRM4SOC2
        output = {}
        for sensortype in ['ES', 'LI', 'LT']:
            print('FRM Processing:', sensortype)
            # Read data
            grp = raw_grps[sensortype]
            raw_data = np.asarray(list(raw_slices[sensortype]["LIGHT"]['data'].values())).transpose()
            raw_dark = np.asarray(list(raw_slices[sensortype]["DARK"]['data'].values())).transpose()

            # read in data for FRM processing
            # raw_data = np.asarray(list(slice.values())).transpose()  # raw_data = np.asarray(grp.getDataset(sensortype).data.tolist())  # dark subtracted signal
            # raw_data = np.asarray(list(slice['data'].values())).transpose()
            # raw_data = np.asarray(grp.getDataset(sensortype).data.tolist())  # dark subtracted signal
            int_time = np.asarray(grp.getDataset("INTTIME").data.tolist())
            int_time = np.mean(int_time)

            # Read FRM characterisation
            radcal_wvl = np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['1'][1:].tolist())
            radcal_cal_raw = pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['2']
            S1 = pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['6']
            S2 = pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['8']
            S1_unc = np.array((pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['7'])[1:].to_list())  # removed /100 as not relative in tartu file
            S2_unc = np.array((pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['9'])[1:].to_list())
            mZ = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_STRAYDATA_LSF").data))
            mZ_unc = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_STRAYDATA_UNCERTAINTY").data))

            # remove 1st line and column, we work on 255 pixel not 256.
            mZ = mZ[1:, 1:]
            mZ_unc = mZ_unc[1:, 1:]

            # set up uncertainty propagation
            mDraws = 100  # number of monte carlo draws
            prop = punpy.MCPropagation(mDraws, parallel_cores=1)
            ind_raw_wvl = (radcal_wvl > 0)  # remove any index for which we do not have radcal wvls available

            mZ = mZ[:, ind_raw_wvl]
            mZ = mZ[ind_raw_wvl, :]
            mZ_unc = mZ_unc[:, ind_raw_wvl]
            mZ_unc = mZ_unc[ind_raw_wvl, :]

            sample_mZ = cm.generate_sample(mDraws, mZ, mZ_unc, "rand")

            Ct = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_TEMPDATA_CAL").data
                                         )[f'{sensortype}_TEMPERATURE_COEFFICIENTS'][1:].tolist())
            Ct_unc = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_TEMPDATA_CAL").data
                                             )[f'{sensortype}_TEMPERATURE_UNCERTAINTIES'][1:].tolist())
            LAMP = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_LAMP").data)['2'])
            LAMP_unc = np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_LAMP").data)['3'])/100*LAMP

            # Defined constants
            # nband = len(radcal_wvl)
            n_iter = 5

            Ct = Ct[ind_raw_wvl]
            Ct_unc = Ct_unc[ind_raw_wvl]

            # uncertainties from data:
            sample_int_time = cm.generate_sample(mDraws, int_time, None, None)
            sample_n_iter = cm.generate_sample(mDraws, n_iter, None, None, dtype=int)
            # sample_mZ = cm.generate_sample(mDraws, mZ, mZ_unc, "rand")
            sample_Ct = cm.generate_sample(mDraws, Ct, Ct_unc, "syst")

            # pad Lamp data and generate sample
            # LAMP = np.pad(LAMP, (0, nband - len(LAMP)), mode='constant')  # PAD with zero if not 255 long
            # LAMP_unc = np.pad(LAMP_unc, (0, nband - len(LAMP_unc)), mode='constant')
            sample_LAMP = cm.generate_sample(mDraws, LAMP, LAMP_unc, "syst")

            # Non-linearity alpha computation
            cal_int = radcal_cal_raw.pop(0)
            radcal_cal = radcal_cal_raw[ind_raw_wvl]
            sample_cal_int = cm.generate_sample(100, cal_int, None, None)

            t1 = S1.iloc[0]
            S1 = S1.drop(S1.index[0])
            t2 = S2.iloc[0]
            S2 = S2.drop(S2.index[0])

            S1 = np.asarray(S1, dtype=float)[ind_raw_wvl]
            S2 = np.asarray(S2, dtype=float)[ind_raw_wvl]
            S1_unc = S1_unc[ind_raw_wvl]
            S2_unc = S2_unc[ind_raw_wvl]

            sample_t1 = cm.generate_sample(mDraws, t1, None, None)
            sample_S1 = cm.generate_sample(mDraws, np.asarray(S1), S1_unc, "rand")
            sample_S2 = cm.generate_sample(mDraws, np.asarray(S2), S2_unc, "rand")

            k = t1/(t2 - t1)
            sample_k = cm.generate_sample(mDraws, k, None, None)
            S12 = self.S12func(k, S1, S2)
            sample_S12 = prop.run_samples(self.S12func, [sample_k, sample_S1, sample_S2])

            if self.sl_method.upper() == 'ZONG':
                sample_n_IB = self.gen_n_IB_sample(mDraws)
                sample_C_zong = prop.run_samples(ProcessL1b_FRMCal.Zong_SL_correction_matrix,
                                                 [sample_mZ, sample_n_IB])
                sample_S12_sl_corr = prop.run_samples(self.Zong_SL_correction, [sample_S12, sample_C_zong])
            else:  # slaper
                sample_S12_sl_corr = self.get_Slaper_Sl_unc(
                    S12, sample_S12, mZ, sample_mZ, n_iter, sample_n_iter, prop, mDraws
                )
            # S12_unc = (prop.process_samples(None, sample_S12_sl_corr)/S12_sl_corr)*100

            # alpha = ((S1-S12)/(S12**2)).tolist()
            alpha = self.alphafunc(S1, S12)
            sample_alpha = prop.run_samples(self.alphafunc, [sample_S1, sample_S12])

            # Updated calibration gain
            if sensortype == "ES":
                ## Irradiance direct and diffuse ratio
                res_py6s = Instrument.read_py6s_model(node)

                ## compute updated radiometric calibration (required step after applying straylight correction)
                sample_updated_radcal_gain = prop.run_samples(self.update_cal_ES,
                                                              [sample_S12_sl_corr, sample_LAMP, sample_cal_int,
                                                               sample_t1])

                ## Compute avg cosine error

                # make zenith angle sample for cosine correction -- read from TU file column header, represents
                # available zenith angles and incurs no uncertainty (hence None, None in generate_sample).
                raw_zen = uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR").attributes["COLUMN_NAMES"].split('\t')[2:]
                zenith_ang = np.asarray([float(x) for x in raw_zen])
                sample_zen_ang = cm.generate_sample(mDraws, zenith_ang, None, None)

                # Note: uncGrp already in scope
                coserror = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype+"_ANGDATA_COSERROR").data))[1:, 2:]
                cos_unc = (np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_UNCERTAINTY").data))[1:, 2:] / 100) * np.abs(coserror)
                coserror_90 = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype+"_ANGDATA_COSERROR_AZ90").data))[1:, 2:]
                cos90_unc = (np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_UNCERTAINTY_AZ90").data))[1:, 2:] / 100) * np.abs(coserror_90)

                # get indexes for first and last radiometric calibration wavelengths in range [300-1000]
                i1 = np.argmin(np.abs(radcal_wvl - 300))
                i2 = np.argmin(np.abs(radcal_wvl - 1000))

                # comparing cos_error for 2 azimuth to check for asymmetry (ideally would be 0)
                azi_avg_coserr = (coserror + coserror_90) / 2.
                # each value has 4 numbers azi = 0, azi = 90, -zen, +zen which need their TU uncertainties combining
                total_coserror_err = np.sqrt(cos_unc**2 + cos90_unc**2 + cos_unc[:, ::-1]**2 + cos90_unc[:, ::-1]**2)

                # comparing cos_error for symetric zenith (ideally would be 0)
                zen_avg_coserr = (azi_avg_coserr + azi_avg_coserr[:, ::-1]) / 2.

                # get total error due to asymmetry  todo: find a smart way to do this without for loops
                tot_asymmetry_err = np.zeros(coserror.shape, float)
                for i in range(255):
                    for j in range(45):
                        tot_asymmetry_err[i, j] = np.std(
                            [coserror[i, j], coserror_90[i, j],
                             coserror[i, -j], coserror_90[i, -j]]
                        )  # get std across the 4 measurements azi_0, azi_90, zen, -zen

                # PDF of total error in cosine, combines TU uncertainties from lab characterisation and asymmetry in
                # cosine response
                zen_unc = np.sqrt(total_coserror_err**2 + tot_asymmetry_err**2)

                # 0 out data that is OOB (out of bounds)
                zen_avg_coserr[0:i1, :] = 0
                zen_avg_coserr[i2:, :] = 0
                zen_unc[0:i1, :] = 0
                zen_unc[i2:, :] = 0

                # use mean and error to build PDF, converting error to uncertainty using Monte Carlo
                sample_zen_avg_coserror = cm.generate_sample(mDraws, zen_avg_coserr, zen_unc, "syst")

                # Compute full hemisperical coserror
                zen0 = np.argmin(np.abs(zenith_ang))
                zen90 = np.argmin(np.abs(zenith_ang - 90))
                deltaZen = (zenith_ang[1::] - zenith_ang[:-1])
                full_hemi_coserror = np.zeros(zen_avg_coserr.shape[0])
                sensitivity_coeff = np.zeros(zen_avg_coserr.shape[0])
                zen_unc_sum = np.zeros(zen_avg_coserr.shape[0])
                for i in range(zen_avg_coserr.shape[0]):
                    full_hemi_coserror[i] = np.sum(
                        zen_avg_coserr[i, zen0:zen90] *
                        np.sin(2 * np.pi * zenith_ang[zen0:zen90] / 180) * deltaZen[zen0:zen90] * np.pi / 180
                    )
                    # calculate the sensitivity coefficient from the LPU
                    sensitivity_coeff[i] = np.sum(
                        np.cos(2 * np.pi * zenith_ang[zen0:zen90] / 180) * deltaZen[zen0:zen90] * np.pi / 180
                    )  # sin(x) differentiates to cos(x)

                    zen_unc_sum[i] = np.sum(zen_unc[i, zen0:zen90])

                # get full hemispherical uncertainty using the LPU
                fhemi_unc = np.sqrt(sensitivity_coeff**2 * zen_unc_sum**2)

                # PDF of full hemispherical cosine error uncertainty
                sample_fhemi_coserr = cm.generate_sample(mDraws, full_hemi_coserror, fhemi_unc, "syst")
            else:
                PANEL = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_PANEL").data)['2'])
                PANEL_unc = (np.asarray(
                    pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_PANEL").data)['3'])/100)*PANEL
                # PANEL = np.pad(PANEL, (0, nband - len(PANEL)), mode='constant')
                # PANEL_unc = np.pad(PANEL_unc, (0, nband - len(PANEL_unc)), mode='constant')
                sample_PANEL = cm.generate_sample(100, PANEL, PANEL_unc, "syst")
                # updated_radcal_gain = self.update_cal_rad(S12_sl_corr, LAMP, PANEL, cal_int, t1)
                sample_updated_radcal_gain = prop.run_samples(self.update_cal_rad,
                                                              [sample_S12_sl_corr, sample_LAMP, sample_PANEL,
                                                               sample_cal_int,
                                                               sample_t1])

            ## sensitivity factor : if gain==0 (or NaN), no calibration is performed and data is affected to 0
            # ind_zero = radcal_cal <= 0
            # ind_nan = np.isnan(radcal_cal)
            # ind_nocal = ind_nan | ind_zero
            # set 1 instead of 0 to perform calibration (otherwise division per 0)
            # updated_radcal_gain[ind_nocal == True] = 1

            # alpha = np.asarray(alpha)
            # Ct = np.asarray(Ct)

            # Filter Raw Data
            # ind_raw_data = (radcal_cal[radcal_wvl > 0]) > 0
            # raw_filtered = np.asarray([raw_data[n][ind_raw_data] for n in range(nmes)])
            # dark_filtered = np.asarray([raw_dark[n][ind_raw_data] for n in range(nmes)])

            ind_zero = radcal_cal <= 0
            ind_nan = np.isnan(radcal_cal)
            ind_nocal = ind_nan | ind_zero
            # set 1 instead of 0 to perform calibration (otherwise division per 0)
            sample_updated_radcal_gain[:, ind_nocal == True] = 1

            data = np.mean(raw_data, axis=0)  # raw data already dark subtracted, use mean for statistical analysis
            data[ind_nocal is True] = 0  # 0 out data outside of cal so it doesn't affect statistics
            dark = np.mean(raw_dark, axis=0)
            dark[ind_nocal is True] = 0
            # data is already 180 len for PML HyperOCR
            # signal uncertainties
            std_light = stats[sensortype]['std_Light']  # standard deviations are taken from generateSensorStats
            std_dark = stats[sensortype]['std_Dark']
            sample_light = cm.generate_sample(100, data, std_light, "rand")
            sample_dark = cm.generate_sample(100, dark, std_dark, "rand")
            sample_dark_corr_data = prop.run_samples(self.dark_Substitution, [sample_light, sample_dark])

            # plt.figure()
            # plt.plot(radcal_wvl, np.mean(sample_light, axis=0))
            # plt.plot(radcal_wvl, np.mean(sample_dark, axis=0), color='k')
            # plt.legend()
            # plt.grid()
            # plt.savefig(f"check_signal_FRM_{sensortype}.png")

            # Non-linearity
            data1 = self.DATA1(data, alpha)  # data*(1 - alpha*data)
            sample_data1 = prop.run_samples(self.DATA1, [sample_dark_corr_data, sample_alpha])

            # data1 unc
            # data1_unc = (prop.process_samples(None, sample_data1)/data1)*100

            # Straylight
            if self.sl_method.upper() == 'ZONG':
                sample_data2 = prop.run_samples(self.Zong_SL_correction, [sample_data1, sample_C_zong])
            else:  # slaper
                sample_data2 = self.get_Slaper_Sl_unc(
                    data1, sample_data1, mZ, sample_mZ, n_iter, sample_n_iter, prop, mDraws
                )


            # data2 unc
            # data2_unc = (prop.process_samples(None, sample_data2)/data2)*100

            # Calibration
            # data3 = self.DATA3(data2, cal_int, int_time, updated_radcal_gain)
            sample_data3 = prop.run_samples(
                self.DATA3, [sample_data2, sample_cal_int, sample_int_time, sample_updated_radcal_gain]
            )

            # thermal
            # data4 = self.DATA4(data3, Ct)
            sample_data4 = prop.run_samples(self.DATA4, [sample_data3, sample_Ct])

            # Cosine correction
            if sensortype == "ES":

                ## ADERU: Py6S results now match the length of input data
                ## I arbitrary select the first value here (index 0). If I understand correctly
                ## this will need to read the stored value in the py6S group instead of recomputing it.
                solar_zenith = np.mean(res_py6s['solar_zenith'], axis=0)
                direct_ratio = np.mean(res_py6s['direct_ratio'][:, 3:], axis=0)
                direct_ratio, _ = self.interp_common_wvls(np.array(direct_ratio, float), res_py6s['wavelengths'], radcal_wvl)

                sample_sol_zen = cm.generate_sample(mDraws, solar_zenith,
                                                    np.asarray([0.05 for i in range(np.size(solar_zenith))]),
                                                    "rand")  # TODO: get second opinion on zen unc in 6S

                # sample_dir_rat = cm.generate_sample(mDraws, direct_ratio[ind_raw_wvl], 0.08*direct_ratio, "syst")
                sample_dir_rat = cm.generate_sample(mDraws, direct_ratio[ind_raw_wvl], 0.08*direct_ratio[ind_raw_wvl], "syst")

                # data5 = self.DATA5(data4, solar_zenith, direct_ratio, zenith_ang, avg_coserror, full_hemi_coserr)
                sample_cos_corr = prop.run_samples(
                    self.get_cos_corr, [sample_zen_ang,
                                        sample_sol_zen,
                                        sample_zen_avg_coserror]
                )
                sample_data5 = prop.run_samples(
                    self.cos_corr, [sample_data4, sample_dir_rat, sample_cos_corr[:,ind_raw_wvl], sample_fhemi_coserr[:,ind_raw_wvl]]
                )
                # sample_data5 = prop.run_samples(self.DATA5, [sample_data4,
                #                                              sample_sol_zen,
                #                                              sample_dir_rat,
                #                                              sample_zen_ang,
                #                                              sample_zen_avg_coserror, # check that zen_avg_coserror is correct
                #                                              sample_fhemi_coserr])

                unc = prop.process_samples(None, sample_data5)
                sample = sample_data5
            else:
                pol = uncGrp.getDataset(f"CLASS_HYPEROCR_{sensortype}_POLDATA_CAL")
                pol.datasetToColumns()
                x = pol.columns['0']
                y = pol.columns['1']
                y_new = np.interp(radcal_wvl, x, y)
                pol.columns['0'] = radcal_wvl
                pol.columns['1'] = y_new

                pol_unc = np.asarray(list(pol.columns['1']))[ind_raw_wvl]  # [1:]
                sample_pol = cm.generate_sample(mDraws, np.ones(len(pol_unc)), pol_unc, "syst")

                sample_pol_mesure = prop.run_samples(self.DATA6, [sample_data4, sample_pol])

                unc = prop.process_samples(None, sample_pol_mesure)
                sample = sample_pol_mesure

            ind_cal = (radcal_cal_raw[ind_raw_wvl]) > 0

            output[f"{sensortype.lower()}Wvls"] = radcal_wvl[ind_raw_wvl == True][ind_cal == True]
            output[f"{sensortype.lower()}Unc"] = unc[ind_cal == True]  # relative uncertainty
            output[f"{sensortype.lower()}Sample"] = sample[:, ind_cal == True]  # samples keep raw

            # sort the outputs ready for following process
            # get sensor specific wavebands to be keys for uncs, then remove from output
            wvls = np.asarray(output.pop(f"{sensortype.lower()}Wvls"), dtype=float)
            _, output[f"{sensortype.lower()}Unc"] = self.interp_common_wvls(
                output[f"{sensortype.lower()}Unc"], wvls, newWaveBands)
            output[f"{sensortype.lower()}Sample"] = self.interpolateSamples(
                output[f"{sensortype.lower()}Sample"], wvls, newWaveBands)

            if ConfigFile.settings['bL2UncertaintyBreakdownPlot']:
                p_unc = UncertaintyGUI(prop)  # initialise plotting obj - punpy MCP as arg
                time = node.attributes['TIME-STAMP'].split(' ')[-2]  # for labelling
                if sensortype.upper() == 'ES':
                    p_unc.plot_unc_from_sample_1D(
                        sample_data5, radcal_wvl, fig_name=f"breakdown_{sensortype}_{time}", name=f"Cosine", xlim=(400, 800)
                    )
                else:
                    p_unc.plot_unc_from_sample_1D(
                        sample_pol_mesure, radcal_wvl, fig_name=f"breakdown_{sensortype}_{time}", name="Polarisation", xlim=(400, 800)
                    )
                p_unc.plot_unc_from_sample_1D(
                    sample_data4, radcal_wvl, fig_name=f"breakdown_{sensortype}_{time}", name=f"Thermal", xlim=(400, 800)
                )
                p_unc.plot_unc_from_sample_1D(
                    sample_data3, radcal_wvl, fig_name=f"breakdown_{sensortype}_{time}", name=f"Calibration", xlim=(400, 800)
                )
                p_unc.plot_unc_from_sample_1D(
                    sample_data2, radcal_wvl, fig_name=f"breakdown_{sensortype}_{time}", name=f"Straylight", xlim=(400, 800)
                )
                p_unc.plot_unc_from_sample_1D(
                    sample_data1, radcal_wvl, fig_name=f"breakdown_{sensortype}_{time}", name=f"Nlin", xlim=(400, 800)
                )
                p_unc.plot_unc_from_sample_1D(
                    sample_dark_corr_data, radcal_wvl, fig_name=f"breakdown_{sensortype}_{time}", name=f"Dark_Corrected", xlim=(400, 800),
                    save={
                        "cal_type": node.attributes["CAL_TYPE"],
                        "time": node.attributes['TIME-STAMP'],
                        "instrument": "SeaBird"
                    }
                )

        return output

    # Measurement Functions
    @staticmethod
    def DATA1(data, alpha):
        return data*(1 - alpha*data)

    @staticmethod
    def DATA3(data2, cal_int, int_time, updated_radcal_gain):
        return data2*(cal_int/int_time)/updated_radcal_gain

    @staticmethod
    def DATA4(data3, Ct):
        data4 = data3*Ct
        data4[data4 <= 0] = 0
        return data4

    @staticmethod
    def DATA5(data4, solar_zenith, direct_ratio, zenith_ang, avg_coserror, full_hemi_coserror):
        ind_closest_zen = np.argmin(np.abs(zenith_ang - solar_zenith))
        cos_corr = (1 - avg_coserror[:, ind_closest_zen]/100)
        Fhcorr = (1 - full_hemi_coserror/100)
        return (direct_ratio*data4*cos_corr) + ((1 - direct_ratio)*data4*Fhcorr)

    @staticmethod
    def DATA6(signal, Cpol):
        return signal*Cpol

    @staticmethod
    def update_cal_ES(S12_sl_corr, LAMP, cal_int, t1):
        return (S12_sl_corr/LAMP)*(10*cal_int/t1)

    @staticmethod
    def update_cal_rad(S12_sl_corr, LAMP, PANEL, cal_int, t1):
        return (np.pi*S12_sl_corr)/(LAMP*PANEL)*(10*cal_int/t1)


class Trios(Instrument):

    warnings.filterwarnings("ignore", message="One of the provided covariance matrix is not positivedefinite. It has been slightly changed")

    def __init__(self):
        super().__init__()   # call to instrument __init__
        self.instrument = 'TriOS-RAMSES'

    def lightDarkStats(self, grp, slice, sensortype):
        raw_cal = grp.getDataset(f"CAL_{sensortype}").data
        raw_back = np.asarray(grp.getDataset("BACK_"+sensortype).data.tolist())
        raw_data = np.asarray(list(slice['data'].values())).transpose()  # data is transpose of old version

        raw_wvl = np.array(pd.DataFrame(grp.getDataset(sensortype).data).columns)
        int_time = np.asarray(grp.getDataset("INTTIME").data.tolist())
        DarkPixelStart = int(grp.attributes["DarkPixelStart"])
        DarkPixelStop = int(grp.attributes["DarkPixelStop"])
        int_time_t0 = int(grp.getDataset(f"BACK_{sensortype}").attributes["IntegrationTime"])

        # sensitivity factor : if raw_cal==0 (or NaN), no calibration is performed and data is affected to 0
        ind_zero = np.array([rc[0] == 0 for rc in raw_cal])  # changed due to raw_cal now being a np array
        ind_nan = np.array([np.isnan(rc[0]) for rc in raw_cal])
        ind_nocal = ind_nan | ind_zero
        raw_cal = np.array([rc[0] for rc in raw_cal])
        raw_cal[ind_nocal==True] = 1

        # check size of data
        nband = len(raw_back)  # indexes changed for raw_back as is brought to L2
        nmes = len(raw_data)
        if nband != len(raw_data[0]):
            print("ERROR: different number of pixels between dat and back")
            return None

        # Data conversion
        mesure = raw_data/65535.0
        calibrated_mesure = np.zeros((nmes, nband))
        calibrated_light_measure = np.zeros((nmes, nband))
        back_mesure = np.zeros((nmes, nband))

        for n in range(nmes):
            # Background correction : B0 and B1 read from "back data"
            back_mesure[n, :] = raw_back[:, 0] + raw_back[:, 1]*(int_time[n]/int_time_t0)
            back_corrected_mesure = mesure[n] - back_mesure[n, :]

            # Offset substraction : dark index read from attribute
            offset = np.mean(back_corrected_mesure[DarkPixelStart:DarkPixelStop])
            offset_corrected_mesure = back_corrected_mesure - offset

            # Normalization for integration time
            normalized_mesure = offset_corrected_mesure*int_time_t0/int_time[n]
            normalised_light_measure = back_corrected_mesure*int_time_t0/int_time[n]  # do not do the dark substitution as we need light data

            # Sensitivity calibration
            calibrated_mesure[n, :] = normalized_mesure  # /raw_cal
            calibrated_light_measure[n, :] = normalised_light_measure  # /raw_cal

        # get light and dark data before correction
        light_avg = np.mean(calibrated_light_measure, axis=0)  # [ind_nocal == False]
        light_std = np.std(calibrated_light_measure, axis=0) / pow(nmes, 0.5)  # [ind_nocal == False]

        # ensure all TriOS outputs are length 255 to match SeaBird HyperOCR stats output
        ones = np.ones(nband)  # to provide array of 1s with the correct shape
        dark_avg = ones * offset
        dark_std = ones * np.std(back_corrected_mesure[DarkPixelStart:DarkPixelStop], axis=0) / pow(nmes, 0.5)
        # adjusting the dark_ave and dark_std shapes will remove sensor specific behaviour in Default and Factory

        stdevSignal = {}
        for i, wvl in enumerate(raw_wvl):
            stdevSignal[wvl] = pow(
                (pow(light_std[i], 2) + pow(dark_std[i], 2)), 0.5) / np.average(calibrated_mesure, axis=0)[i]

        return dict(
            ave_Light=np.array(light_avg),
            ave_Dark=np.array(dark_avg),
            std_Light=np.array(light_std),
            std_Dark=np.array(dark_std),
            std_Signal=stdevSignal,
        )

    def FRM(self, node, uncGrp, raw_grps, raw_slices, stats, newWaveBands):
        # TriOS specific
        output = {}
        stats = None  # stats is unused in this method, but required as an input because of Seabird
        for sensortype in ['ES', 'LI', 'LT']:

            ### Read HDF file inputs
            grp = raw_grps[sensortype]
            # slice = rawSlices[sensortype]
            slice = raw_slices[sensortype]

            # read data for L1B FRM processing
            raw_data = np.asarray(list(slice['data'].values())).transpose()  # raw_data = np.asarray(grp.getDataset(sensortype).data.tolist())
            DarkPixelStart = int(grp.attributes["DarkPixelStart"])
            DarkPixelStop = int(grp.attributes["DarkPixelStop"])
            int_time = np.asarray(grp.getDataset("INTTIME").data.tolist())
            int_time_t0 = int(grp.getDataset("BACK_" + sensortype).attributes["IntegrationTime"])

            ### Read full characterisation files
            radcal_wvl = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['1'][1:].tolist())

            ### for masking arrays only
            raw_cal = grp.getDataset(f"CAL_{sensortype}").data

            B0 = np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['4'][1:].tolist())
            B1 = np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['5'][1:].tolist())
            S1 = pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['6']
            S2 = pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['8']
            mZ = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_STRAYDATA_LSF").data))
            mZ_unc = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_STRAYDATA_UNCERTAINTY").data))
            mZ = mZ[1:, 1:]  # remove 1st line and column, we work on 255 pixel not 256.
            mZ_unc = mZ_unc[1:, 1:]  # remove 1st line and column, we work on 255 pixel not 256.
            Ct = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_TEMPDATA_CAL").data[1:].transpose().tolist())[4])
            Ct_unc = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_TEMPDATA_CAL").data[1:].transpose().tolist())[5])

            # Convert TriOS mW/m2/nm to uW/cm^2/nm
            LAMP = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_LAMP").data)['2']) / 10  # div by 10
            # corrects LAMP and LAMP_unc
            LAMP_unc = (np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_LAMP").data)['3'])/100)*LAMP

            # Defined constants
            nband = len(B0)
            nmes = len(raw_data)
            grp.attributes["nmes"] = nmes
            n_iter = 5

            # set up uncertainty propagation
            mDraws = 100  # number of monte carlo draws
            prop = punpy.MCPropagation(mDraws, parallel_cores=1)

            # uncertainties from data:
            sample_mZ = cm.generate_sample(mDraws, mZ, mZ_unc, "rand")

            sample_n_iter = cm.generate_sample(mDraws, n_iter, None, None, dtype=int)
            sample_int_time_t0 = cm.generate_sample(mDraws, int_time_t0, None, None)
            sample_LAMP = cm.generate_sample(mDraws, LAMP, LAMP_unc, "syst")
            sample_Ct = cm.generate_sample(mDraws, Ct, Ct_unc, "syst")

            # Non-linearity alpha computation

            t1 = S1.iloc[0]
            S1 = S1.drop(S1.index[0])
            t2 = S2.iloc[0]
            S2 = S2.drop(S2.index[0])
            sample_t1 = cm.generate_sample(mDraws, t1, None, None)

            S1 = np.asarray(S1/65535.0, dtype=float)
            S2 = np.asarray(S2/65535.0, dtype=float)
            k = t1/(t2 - t1)
            sample_k = cm.generate_sample(mDraws, k, None, None)

            S1_unc = (pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['7'])[1:]
            S2_unc = (pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['9'])[1:]
            S1_unc = np.asarray(S1_unc/65535.0, dtype=float)
            S2_unc = np.asarray(S2_unc/65535.0, dtype=float)  # put in the same units as S1/S2

            sample_S1 = cm.generate_sample(mDraws, np.asarray(S1), S1_unc, "rand")
            sample_S2 = cm.generate_sample(mDraws, np.asarray(S2), S2_unc, "rand")

            S12 = self.S12func(k, S1, S2)
            sample_S12 = prop.run_samples(self.S12func, [sample_k, sample_S1, sample_S2])

            if self.sl_method.upper() == 'ZONG':  # for internal coding use only, set by default in HCP
                sample_n_IB = self.gen_n_IB_sample(mDraws)  # n_IB sample must be integer and in the range 3-6
                sample_C_zong = prop.run_samples(ProcessL1b_FRMCal.Zong_SL_correction_matrix,
                                                 [sample_mZ, sample_n_IB])
                sample_S12_sl_corr = prop.run_samples(self.Zong_SL_correction, [sample_S12, sample_C_zong])
            else:  # slaper
                sample_S12_sl_corr = self.get_Slaper_Sl_unc(
                    S12, sample_S12, mZ, sample_mZ, n_iter, sample_n_iter, prop, mDraws
                )

            alpha = self.alphafunc(S1, S12)
            sample_alpha = prop.run_samples(self.alphafunc, [sample_S1, sample_S12])

            # Updated calibration gain
            if sensortype == "ES":
                # Irradiance direct and diffuse ratio
                # res_py6s = ProcessL1b_FRMCal.get_direct_irradiance_ratio(node, sensortype, called_L2=True)
                res_py6s = Instrument.read_py6s_model(node)

                # updated_radcal_gain = self.update_cal_ES(S12_sl_corr, LAMP, int_time_t0, t1)
                sample_updated_radcal_gain = prop.run_samples(self.update_cal_ES,
                                                              [sample_S12_sl_corr, sample_LAMP, sample_int_time_t0,
                                                               sample_t1])
                ## Compute avg cosine error

                # make zenith angle sample for cosine correction -- read from TU file column header, represents
                # available zenith angles and incurs no uncertainty (hence None, None in generate_sample).
                raw_zen = uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR").attributes["COLUMN_NAMES"].split('\t')[2:]
                zenith_ang = np.asarray([float(x) for x in raw_zen])
                sample_zen_ang = cm.generate_sample(mDraws, zenith_ang, None, None)

                # Note: uncGrp already in scope
                coserror = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR").data))[1:, 2:]
                cos_unc = (np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_UNCERTAINTY").data))[1:,
                           2:] / 100) * np.abs(coserror)
                coserror_90 = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR_AZ90").data))[
                              1:, 2:]
                cos90_unc = (np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_UNCERTAINTY_AZ90").data))[
                             1:, 2:] / 100) * np.abs(coserror_90)

                # get indexes for first and last radiometric calibration wavelengths in range [300-1000]
                i1 = np.argmin(np.abs(radcal_wvl - 300))
                i2 = np.argmin(np.abs(radcal_wvl - 1000))

                # comparing cos_error for 2 azimuth to check for asymmetry (ideally would be 0)
                azi_avg_coserr = (coserror + coserror_90) / 2.
                # each value has 4 numbers azi = 0, azi = 90, -zen, +zen which need their TU uncertainties combining
                total_coserror_err = np.sqrt(
                    cos_unc ** 2 + cos90_unc ** 2 + cos_unc[:, ::-1] ** 2 + cos90_unc[:, ::-1] ** 2)

                # comparing cos_error for symetric zenith (ideally would be 0)
                zen_avg_coserr = (azi_avg_coserr + azi_avg_coserr[:, ::-1]) / 2.

                # get total error due to asymmetry  todo: find a smart way to do this without for loops
                tot_asymmetry_err = np.zeros(coserror.shape, float)
                for i in range(255):
                    for j in range(45):
                        tot_asymmetry_err[i, j] = np.std(
                            [coserror[i, j], coserror_90[i, j],
                             coserror[i, -j], coserror_90[i, -j]]
                        )  # get std across the 4 measurements azi_0, azi_90, zen, -zen

                # PDF of total error in cosine, combines TU uncertainties from lab characterisation and asymmetry in
                # cosine response
                zen_unc = np.sqrt(total_coserror_err ** 2 + tot_asymmetry_err ** 2)

                # 0 out data that is OOB (out of bounds)
                zen_avg_coserr[0:i1, :] = 0
                zen_avg_coserr[i2:, :] = 0
                zen_unc[0:i1, :] = 0
                zen_unc[i2:, :] = 0

                # use mean and error to build PDF, converting error to uncertainty using Monte Carlo
                sample_zen_avg_coserror = cm.generate_sample(mDraws, zen_avg_coserr, zen_unc, "syst")

                # Compute full hemisperical coserror
                zen0 = np.argmin(np.abs(zenith_ang))
                zen90 = np.argmin(np.abs(zenith_ang - 90))
                deltaZen = (zenith_ang[1::] - zenith_ang[:-1])
                full_hemi_coserror = np.zeros(zen_avg_coserr.shape[0])
                sensitivity_coeff = np.zeros(zen_avg_coserr.shape[0])
                zen_unc_sum = np.zeros(zen_avg_coserr.shape[0])
                for i in range(zen_avg_coserr.shape[0]):
                    full_hemi_coserror[i] = np.sum(
                        zen_avg_coserr[i, zen0:zen90] *
                        np.sin(2 * np.pi * zenith_ang[zen0:zen90] / 180) * deltaZen[zen0:zen90] * np.pi / 180
                    )
                    # calculate the sensitivity coefficient from the LPU
                    sensitivity_coeff[i] = np.sum(
                        np.cos(2 * np.pi * zenith_ang[zen0:zen90] / 180) * deltaZen[zen0:zen90] * np.pi / 180
                    )  # sin(x) differentiates to cos(x)

                    zen_unc_sum[i] = np.sum(zen_unc[i, zen0:zen90])

                # get full hemispherical uncertainty using the LPU
                fhemi_unc = np.sqrt(sensitivity_coeff ** 2 * zen_unc_sum ** 2)

                # PDF of full hemispherical cosine error uncertainty
                sample_fhemi_coserr = cm.generate_sample(mDraws, full_hemi_coserror, fhemi_unc, "syst")

                # I was doing some debugging here, sorry that this ended up in the PR.
                # p_unc = UncertaintyGUI(prop)
                # p_unc.plot_unc_from_sample_1D(sample_zen_avg_coserror, radcal_wvl, "zen")
                # p_unc.plot_unc_from_sample_1D(sample_fhemi_coserr, radcal_wvl, "fhemi")
            else:
                PANEL = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_PANEL").data)['2'])
                unc_PANEL = (np.asarray(
                    pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_PANEL").data)['3'])/100)*PANEL
                sample_PANEL = cm.generate_sample(mDraws, PANEL, unc_PANEL, "syst")
                # updated_radcal_gain = self.update_cal_rad(PANEL, S12_sl_corr, LAMP, int_time_t0, t1)
                sample_updated_radcal_gain = prop.run_samples(self.update_cal_rad,
                                                              [sample_PANEL, sample_S12_sl_corr, sample_LAMP,
                                                               sample_int_time_t0, sample_t1])

            # Data conversion
            mesure = raw_data/65535.0

            back_mesure = np.array([B0 + B1*(int_time[n]/int_time_t0) for n in range(nmes)])
            back_corrected_mesure = mesure - back_mesure
            std_light = np.std(back_corrected_mesure, axis=0)/nmes
            sample_back_corrected_mesure = cm.generate_sample(mDraws, np.mean(back_corrected_mesure, axis=0), std_light,
                                                              "rand")

            # Offset substraction : dark index read from attribute
            offset = np.mean(back_corrected_mesure[:, DarkPixelStart:DarkPixelStop], axis=1)
            offset_corrected_mesure = np.asarray(
                [back_corrected_mesure[:, i] - offset for i in range(nband)]).transpose()
            offset_std = np.std(back_corrected_mesure[:, DarkPixelStart:DarkPixelStop], axis=1)  # std in dark pixels
            std_dark = np.power((np.power(np.std(offset), 2) + np.power(offset_std, 2))/np.power(nmes, 2), 0.5)

            # add in quadrature with std in offset across scans
            sample_offset = cm.generate_sample(mDraws, np.mean(offset), np.mean(std_dark), "rand")
            sample_offset_corrected_mesure = prop.run_samples(self.dark_Substitution,
                                                              [sample_back_corrected_mesure, sample_offset])

            # average the signal and int_time for the station
            offset_corr_mesure = np.mean(offset_corrected_mesure, axis=0)
            int_time = np.average(int_time)

            prop = punpy.MCPropagation(mDraws, parallel_cores=1)

            # set standard variables
            # n_iter = 5
            # sample_n_iter = cm.generate_sample(mDraws, n_iter, None, None, dtype=int)

            # Non-Linearity Correction
            linear_corr_mesure = self.non_linearity_corr(offset_corr_mesure, alpha)
            sample_linear_corr_mesure = prop.run_samples(self.non_linearity_corr,
                                                         [sample_offset_corrected_mesure, sample_alpha])

            # Straylight Correction
            if self.sl_method.upper() == 'ZONG':  # for internal use only, Zong set as default in HCP
                sample_straylight_corr_mesure = prop.run_samples(
                    self.Zong_SL_correction, [sample_linear_corr_mesure, sample_C_zong]
                )
            else:
                sample_straylight_corr_mesure = self.get_Slaper_Sl_unc(
                    linear_corr_mesure, sample_linear_corr_mesure, mZ, sample_mZ, n_iter, sample_n_iter, prop, mDraws
                )   # simplified code by adding Slaper correction to one fucntion

            # Normalization Correction, based on integration time
            sample_normalized_mesure = sample_straylight_corr_mesure*int_time_t0/int_time

            # Calculate New Calibration Coeffs
            sample_calibrated_mesure = prop.run_samples(self.absolute_calibration,
                                                        [sample_normalized_mesure, sample_updated_radcal_gain])

            # Thermal correction
            sample_thermal_corr_mesure = prop.run_samples(self.thermal_corr, [sample_Ct, sample_calibrated_mesure])

            if sensortype.lower() == "es":
                # get cosine correction attributes and samples from dictionary

                ## ADERU: Py6S results now match the length of input data
                ## I arbitrary select the first value here (index 0). If I understand correctly
                ## this will need to read the stored value in the py6S group instead of recomputing it.
                solar_zenith = np.mean(res_py6s['solar_zenith'], axis=0)
                direct_ratio = np.mean(res_py6s['direct_ratio'][:, 2:], axis=0)
                direct_ratio, _ = self.interp_common_wvls(direct_ratio, res_py6s['wavelengths'], radcal_wvl)
                sample_sol_zen = cm.generate_sample(mDraws, solar_zenith, 0.05, "rand")
                sample_dir_rat = cm.generate_sample(mDraws, direct_ratio, 0.08*direct_ratio, "syst")
                sample_cos_corr = prop.run_samples(
                    self.get_cos_corr, [sample_zen_ang,
                                        sample_sol_zen,
                                        sample_zen_avg_coserror]
                )
                sample_cos_corr_mesure = prop.run_samples(
                    self.cos_corr, [sample_thermal_corr_mesure, sample_dir_rat, sample_cos_corr, sample_fhemi_coserr]
                )
                # sample_cos_corr_mesure = prop.run_samples(self.cosine_corr,
                #                                           [sample_zen_avg_coserror, sample_fhemi_coserr, sample_zen_ang,
                #                                            sample_thermal_corr_mesure, sample_sol_zen, sample_dir_rat])

                sample = sample_cos_corr_mesure
                unc = prop.process_samples(None, sample_cos_corr_mesure)
            else:
                # read pol uncertainties and interpolate to radcal wavebands
                pol = uncGrp.getDataset(f"CLASS_RAMSES_{sensortype}_POLDATA_CAL")
                pol.datasetToColumns()
                x = pol.columns['0']
                y = pol.columns['1']
                y_new = np.interp(radcal_wvl, x, y)
                pol.columns['0'] = radcal_wvl
                pol.columns['1'] = y_new

                pol_unc = np.asarray(list(pol.columns['1']))
                sample_pol = cm.generate_sample(mDraws, np.ones(len(pol_unc)), pol_unc, "syst")

                sample_pol_mesure = prop.run_samples(self.CPOL_MF, [sample_thermal_corr_mesure, sample_pol])

                sample = sample_pol_mesure
                unc = prop.process_samples(None, sample_pol_mesure)

            # mask for arrays
            ind_zero = np.array([rc[0] == 0 for rc in raw_cal])  # changed due to raw_cal now being a np array
            ind_nan = np.array([np.isnan(rc[0]) for rc in raw_cal])
            ind_nocal = ind_nan | ind_zero

            # Remove wvl without calibration from the dataset and make uncertainties relative
            output[f"{sensortype.lower()}Wvls"] = radcal_wvl[ind_nocal == False]
            output[
                f"{sensortype.lower()}Unc"] = unc[ind_nocal == False]  # dict(zip(str_wvl[self.ind_nocal==False], filtered_unc))  # unc in dict with wavelengths
            output[f"{sensortype.lower()}Sample"] = sample[:, ind_nocal == False]  # samples keep raw

        for sensortype in ['ES', 'LI', 'LT']:
            # get sensor specific wavebands - output[f"{sensortype.lower()}Wvls"].pop
            wvls = np.asarray(output.pop(f"{sensortype.lower()}Wvls"), dtype=float)
            _, output[f"{sensortype.lower()}Unc"] = self.interp_common_wvls(
                output[f"{sensortype.lower()}Unc"], wvls, newWaveBands)
            output[f"{sensortype.lower()}Sample"] = self.interpolateSamples(
                output[f"{sensortype.lower()}Sample"], wvls, newWaveBands)

            if ConfigFile.settings['bL2UncertaintyBreakdownPlot']:
                p_unc = UncertaintyGUI(prop)  # initialise plotting obj - punpy MCP as arg
                time = ' '.join(node.attributes['TIME-STAMP'][0:-1].split('T'))  # time string for labelling
                if sensortype.upper() == 'ES':
                    p_unc.plot_unc_from_sample_1D(
                        sample_cos_corr_mesure, radcal_wvl, fig_name=f"breakdown_{sensortype}_{time}", name=f"Cosine", xlim=(400, 800)
                    )
                else:
                    p_unc.plot_unc_from_sample_1D(
                        sample_pol_mesure, radcal_wvl, fig_name=f"breakdown_{sensortype}_{time}", name="Polarisation", xlim=(400, 800)
                    )
                p_unc.plot_unc_from_sample_1D(
                    sample_thermal_corr_mesure, radcal_wvl, fig_name=f"breakdown_{sensortype}_{time}", name=f"Thermal", xlim=(400, 800)
                )
                p_unc.plot_unc_from_sample_1D(
                    sample_calibrated_mesure, radcal_wvl, fig_name=f"breakdown_{sensortype}_{time}", name=f"Calibration", xlim=(400, 800)
                )
                p_unc.plot_unc_from_sample_1D(
                    sample_straylight_corr_mesure, radcal_wvl, fig_name=f"breakdown_{sensortype}_{time}", name=f"Straylight", xlim=(400, 800)
                )
                p_unc.plot_unc_from_sample_1D(
                    sample_linear_corr_mesure, radcal_wvl, fig_name=f"breakdown_{sensortype}_{time}", name=f"Nlin", xlim=(400, 800)
                )
                p_unc.plot_unc_from_sample_1D(
                    sample_offset_corrected_mesure, radcal_wvl, fig_name=f"breakdown_{sensortype}_{time}", name=f"Dark_Corrected", xlim=(400, 800),
                    save={
                        "cal_type": node.attributes["CAL_TYPE"],
                        "time": node.attributes['TIME-STAMP'],
                        "instrument": "TriOS"
                    }
                )

        return output  # return products as dictionary to be appended to xSlice

    # Measurement functions
    @staticmethod
    def back_Mesure(B0, B1, int_time, t0):
        return B0 + B1*(int_time/t0)

    @staticmethod
    def CPOL_MF(signal, Cpol):
        return signal*Cpol

    @staticmethod
    def update_cal_ES(S12_sl_corr, LAMP, int_time_t0, t1):
        updated_radcal_gain = (S12_sl_corr/LAMP)*(int_time_t0/t1)
        # sensitivity factor : if gain==0 (or NaN), no calibration is performed and data is affected to 0
        ind_zero = (updated_radcal_gain <= 1e-2)
        ind_nan = np.isnan(updated_radcal_gain)
        ind_nocal = ind_nan | ind_zero
        updated_radcal_gain[ind_nocal == True] = 1  # set 1 instead of 0 to perform calibration (otherwise division per 0)
        return updated_radcal_gain

    @staticmethod
    def update_cal_rad(PANEL, S12_sl_corr, LAMP, int_time_t0, t1):
        updated_radcal_gain = (np.pi*S12_sl_corr)/(LAMP*PANEL)*(int_time_t0/t1)

        # sensitivity factor : if gain==0 (or NaN), no calibration is performed and data is affected to 0
        ind_zero = (updated_radcal_gain <= 1e-2)
        ind_nan = np.isnan(updated_radcal_gain)
        ind_nocal = ind_nan | ind_zero
        updated_radcal_gain[
            ind_nocal == True] = 1  # set 1 instead of 0 to perform calibration (otherwise division per 0)
        return updated_radcal_gain
